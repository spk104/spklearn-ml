{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Day 30 - Class </h1>\n",
    "\n",
    "## Covariance\n",
    "\n",
    "The covariance is a measure for how two variables are related to each other, i.e., how two variables vary with each other.\n",
    "\n",
    "Let n\n",
    "be the population size, x and y two different features (variables), and μ\n",
    "\n",
    "the population mean; the covariance can then be formally defined as:\n",
    "\n",
    "<img src='img/covariance-01.png'/>\n",
    "\n",
    "A covariance of 0 indicates that two variables are totally unrelated. If the covariance is positive, the variables increase in the same direction, and if the covariance is negative, the variables change in opposite directions\n",
    "\n",
    "Pearson’s ρ or “r” (or typically just called “correlation coefficient”) is measures the linear correlation between two features and is closely related to the covariance. In fact, it’s a normalized version of the covariance as shown below:\n",
    "\n",
    "<img src='img/covariance-02.png'/>\n",
    "\n",
    "By dividing the covariance by the features’ standard deviations, we ensure that the correlation between two features is in the range [-1, 1], which makes it more interpretable than the unbounded covariance. However, note that the covariance and correlation are exactly the same if the features are normalized to unit variance (e.g., via standardization or z-score normalization). Two features are perfectly positively correlated if ρ=1 and pefectly negatively correlated if ρ=−1. No correlation is observed if ρ=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "[3. 4.]\n",
      "\n",
      "[[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "\n",
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "\n",
      "[8. 0.]\n",
      "\n",
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "print()\n",
    "# calculate the mean of each column\n",
    "M = mean(A.T, axis=1)\n",
    "print(M)\n",
    "print()\n",
    "# center columns by subtracting column means\n",
    "C = A - M\n",
    "print(C)\n",
    "print()\n",
    "# calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "print(V)\n",
    "print()\n",
    "# eigendecomposition of covariance matrix\n",
    "values, vectors = eig(V)\n",
    "print(vectors)\n",
    "print()\n",
    "print(values)\n",
    "print()\n",
    "# project data\n",
    "P = vectors.T.dot(C.T)\n",
    "print(P.T)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the final output above we coudl take only PCA-1 component for our further analysis.. thereby reducing the dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n",
      "[8.00000000e+00 2.25080839e-33]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.82842712e+00,  2.22044605e-16],\n",
       "       [ 0.00000000e+00,  0.00000000e+00],\n",
       "       [ 2.82842712e+00, -2.22044605e-16]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn way of coding the same\n",
    "from sklearn.decomposition import PCA\n",
    "A = array([[1,2],[3,4],[5,6]])\n",
    "print(A)\n",
    "pca = PCA()\n",
    "pca.fit(A)\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "B = pca.transform(A)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[0.70710678 0.70710678]]\n",
      "[8.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.82842712],\n",
       "       [ 0.        ],\n",
       "       [ 2.82842712]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn way of coding the same\n",
    "from sklearn.decomposition import PCA\n",
    "A = array([[1,2],[3,4],[5,6]])\n",
    "print(A)\n",
    "pca = PCA(1)\n",
    "pca.fit(A)\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "B = pca.transform(A)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADABoost\n",
    "\n",
    "A tree with just one node and two leaves is called as stump. In ADA Boost we create a forest of stumps. Since we are only making use of one feature at a time, stumps will act as a weak learner, just like how ADA Boost wants it.\n",
    "\n",
    "<img src='img/adaboost-01.png'/>\n",
    "\n",
    "In Random Forest each decision tree gets equal weight on the vote, for doing the final classification. Wheras in ADABoost, some stumps get a higher weightage as compared to others.\n",
    "\n",
    "In Random Forest the order in which the trees are formed aren't important, wheras in ADABoost, order is important while making the forest of stumps.\n",
    "\n",
    "<img src='img/adaboost-02.png'/>\n",
    "\n",
    "<img src='img/adaboost-03.png'/>\n",
    "\n",
    "<img src='img/adaboost-04.png'/>\n",
    "\n",
    "<img src='img/adaboost-05.png'/>\n",
    "\n",
    "<img src='img/adaboost-06.png'/>\n",
    "\n",
    "<img src='img/adaboost-07.png'/>\n",
    "\n",
    "<img src='img/adaboost-08.png'/>\n",
    "\n",
    "<img src='img/adaboost-09.png'/>\n",
    "\n",
    "We find GINI index for each of the features\n",
    "\n",
    "<img src='img/adaboost-10.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patient weight is the weakest learner, so this will be the first stump in the forest. Now we have to determine how much say this stump will have in the final classification. \n",
    "\n",
    "<img src='img/adaboost-11.png'/>\n",
    "\n",
    "<img src='img/adaboost-12.png'/>\n",
    "\n",
    "When the total error is higher, amount of say is lower and vice versa\n",
    "<img src='img/adaboost-13.png'/>\n",
    "\n",
    "<img src='img/adaboost-14.png'/>\n",
    "\n",
    "<img src='img/adaboost-15.png'/>\n",
    "\n",
    "<img src='img/adaboost-16.png'/>\n",
    "\n",
    "<img src='img/adaboost-17.png'/>\n",
    "\n",
    "<img src='img/adaboost-18.png'/>\n",
    "\n",
    "\n",
    "For the <b>incorrectly</b> classified samples, we <b> INCREASE </b> the sample weight in the next iteration\n",
    "\n",
    "<img src='img/adaboost-19.png'/>\n",
    "\n",
    "<img src='img/adaboost-20.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the <b>correcly</b> classified samples, we <b> DECREASE </b> the sample weight in the next iteration\n",
    "\n",
    "<img src='img/adaboost-21.png' />\n",
    "\n",
    "<img src='img/adaboost-22.png' />\n",
    "\n",
    "<img src='img/adaboost-23.png' />\n",
    "\n",
    "<img src='img/adaboost-24.png' />\n",
    "\n",
    "<img src='img/adaboost-25.png' />\n",
    "\n",
    "<img src='img/adaboost-26.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/adaboost-27.png' />\n",
    "\n",
    "<img src='img/adaboost-28.png' />\n",
    "\n",
    "<img src='img/adaboost-29.png' />\n",
    "\n",
    "<img src='img/adaboost-30.png' />\n",
    "\n",
    "<img src='img/adaboost-31.png' />\n",
    "\n",
    "<img src='img/adaboost-32.png' />\n",
    "\n",
    "<img src='img/adaboost-33.png' />\n",
    "\n",
    "<img src='img/adaboost-34.png' />\n",
    "\n",
    "<img src='img/adaboost-35.png' />\n",
    "\n",
    "<img src='img/adaboost-36.png' />\n",
    "\n",
    "<img src='img/adaboost-37.png' />\n",
    "\n",
    "<img src='img/adaboost-38.png' />\n",
    "\n",
    "Imagine the prediction is made for whether the person has heart disease or not, based on how each stump classify the output, we find the amount of say for each stump that say 'yes' v/s the one's that says 'no'. The sum of all 'yes' is checked with the sum of all 'no'\n",
    "<img src='img/adaboost-39.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting - Regression\n",
    "\n",
    "<img src='img/gbr-01.png'/>\n",
    "\n",
    "this leaf represents an initial guess for the Weights of all the samples\n",
    "\n",
    "<img src='img/gbr-02.png'/>\n",
    "\n",
    "<img src='img/gbr-03.png'/>\n",
    "\n",
    "Also, like AdaBoost, Gradient Boost scales the trees. However Gradient Boost scales all trees by the same amount.\n",
    "\n",
    "<img src='img/gbr-04.png'/>\n",
    "\n",
    "<img src='img/gbr-05.png'/>\n",
    "\n",
    "<img src='img/gbr-06.png'/>\n",
    "\n",
    "<img src='img/gbr-07.png'/>\n",
    "\n",
    "It might seem strange to predict the residuals instead of the weight, but let's just go with the flow\n",
    "\n",
    "<img src='img/gbr-08.png'/>\n",
    "\n",
    "In this example we are only allowing upto 4 leaves, but when using a larger dataset, it is common to allow anywhere from 8 to 32. Because we are restricting the number of leaves, some residuals will be clubbed into a single leaf. We take the average of the multiple residuals in a single leaf.\n",
    "\n",
    "<img src='img/gbr-09.png'/>\n",
    "\n",
    "<img src='img/gbr-10.png'/>\n",
    "\n",
    "<img src='img/gbr-11.png'/>\n",
    "\n",
    "Is this great ? No. Model fits the training data too well. In other words, we have low Bias but probably very high Variance\n",
    "\n",
    "<img src='img/gbr-12.png'/>\n",
    "\n",
    "<img src='img/gbr-13.png'/>\n",
    "\n",
    "But it's a little bit better than the Prediction made with just the original leaf, which predicted that all samples would weigh 71.2. According to the person who invented Gradient Boost, Jerome Friedman, empirical evidence shows that taking lots of small steps(a.k.a learning rate) in the right direction results in better Predictions with a Testing Dataset, i.e. lower Variance \n",
    "\n",
    "Now fill the residual columns with the new predictions\n",
    "\n",
    "<img src='img/gbr-14.png'/>\n",
    "\n",
    "We build a new tree in the next iteration\n",
    "\n",
    "<img src='img/gbr-15.png'/>\n",
    "\n",
    "<img src='img/gbr-16.png'/>\n",
    "\n",
    "<img src='img/gbr-17.png'/>\n",
    "\n",
    "<img src='img/gbr-18.png'/>\n",
    "\n",
    "<img src='img/gbr-19.png'/>\n",
    "\n",
    "We keep making trees until we reach the maximum specified, or adding additional trees does not significantly reduce the size of the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting - Classification\n",
    "\n",
    "<img src='img/gbc-01.png'/>\n",
    "<img src='img/gbc-02.png'/>\n",
    "<img src='img/gbc-03.png'/>\n",
    "<img src='img/gbc-04.png'/>\n",
    "<img src='img/gbc-05.png'/>\n",
    "<img src='img/gbc-06.png'/>\n",
    "<img src='img/gbc-07.png'/>\n",
    "<img src='img/gbc-08.png'/>\n",
    "<img src='img/gbc-09.png'/>\n",
    "<img src='img/gbc-10.png'/>\n",
    "<img src='img/gbc-11.png'/>\n",
    "<img src='img/gbc-12.png'/>\n",
    "<img src='img/gbc-13.png'/>\n",
    "<img src='img/gbc-14.png'/>\n",
    "<img src='img/gbc-15.png'/>\n",
    "<img src='img/gbc-16.png'/>\n",
    "<img src='img/gbc-17.png'/>\n",
    "<img src='img/gbc-18.png'/>\n",
    "<img src='img/gbc-19.png'/>\n",
    "<img src='img/gbc-20.png'/>\n",
    "<img src='img/gbc-21.png'/>\n",
    "<img src='img/gbc-22.png'/>\n",
    "<img src='img/gbc-23.png'/>\n",
    "<img src='img/gbc-24.png'/>\n",
    "<img src='img/gbc-25.png'/>\n",
    "<img src='img/gbc-26.png'/>\n",
    "<img src='img/gbc-27.png'/>\n",
    "<img src='img/gbc-28.png'/>\n",
    "<img src='img/gbc-29.png'/>\n",
    "<img src='img/gbc-30.png'/>\n",
    "<img src='img/gbc-31.png'/>\n",
    "<img src='img/gbc-32.png'/>\n",
    "<img src='img/gbc-33.png'/>\n",
    "<img src='img/gbc-34.png'/>\n",
    "<img src='img/gbc-35.png'/>\n",
    "<img src='img/gbc-36.png'/>\n",
    "<img src='img/gbc-37.png'/>\n",
    "<img src='img/gbc-38.png'/>\n",
    "<img src='img/gbc-39.png'/>\n",
    "<img src='img/gbc-40.png'/>\n",
    "<img src='img/gbc-41.png'/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA\n",
    "ADA Boosting\n",
    "Gradient boosting regression\n",
    "Gradient boosting classification\n",
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://sebastianraschka.com/faq/docs/covariance-vs-correlation.html\n",
    "\n",
    "https://machinelearningmastery.com/adaboost-ensemble-in-python/\n",
    "\n",
    "https://www.youtube.com/watch?v=LsK-xG1cLYA\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
