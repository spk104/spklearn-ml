{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Day 24 - Class </h1>\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Logistic Regression is used when the dependent variable (target or y variable) is categorical.\n",
    "\n",
    "e.g\n",
    "- predict an email is spam or not\n",
    "- predict whether it's going to rain tomorrow or not\n",
    "\n",
    "We cannot use linear regression for these kind of problems as linear regressio is unbounded. To solve these kind of problems is where Logistic Regression is used. \n",
    "\n",
    "Instead of fitting a line to the data (as was the case in linear regression), logistic regression fits an 'S' shaped logistic function\n",
    "\n",
    "<img src='img/logistic-reg-1.png'/>\n",
    "\n",
    "When the weights are closer to 0, y value indicates \"Not Obese\" and as the weight increases the y value slowly starts moving upwards to the \"Obese\" category.\n",
    "\n",
    "Curve goes from 0 to 1\n",
    "\n",
    "<img src='img/logistic-reg-2.png'/>\n",
    "\n",
    "Consider a new point in the X-axis, there is a high probability that mouse is Obese\n",
    "\n",
    "<img src='img/logistic-reg-3.png'/>\n",
    "\n",
    "If we pick a point in the X-axis kind of in the middle, then 50% probability for the mouse to be Obese and so on.\n",
    "\n",
    "Just like linear regression, logistic regression can also work with multiple independent variables. And the independent variables can be categorical or continous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear regression, we fit the line using \"least squares\" method. i.e. We minimize the sum of the squares of the residuals. We also use R¬≤ to compare simple models to complicated models.\n",
    "\n",
    "Logistic regression doesnt have any residuals and hence no least squares or R¬≤, instead it uses \"maximum likelihood\". Curve with the maximum likelihood is selected. We try fitting the curve that covers the maximum cordinates. \n",
    "\n",
    "In Linear regression, when we try the best fit line , the y value ranges from -infinity to +infinity. \n",
    "In Logistic regression, the y value as seen in the plot above ranges between 0 - 1 i.e. the probability values and that's a problem if we are trying to solve Logistic regression using Linear Models (Generalized linear models)\n",
    "<img src='img/logistic-reg-4.png'/>\n",
    "\n",
    "To solve the problem we can transform the y-axis from probability of obesity to the log(odds of obesity) as shown int he picture below\n",
    "<img src='img/logistic-reg-5.png'/>\n",
    "\n",
    "This transformation can be done using the <b>logit</b> function\n",
    "\n",
    "<img src='img/logistic-reg-6.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithms - A quick refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logarithm-1.png'/>\n",
    "\n",
    "<img src='img/logarithm-2.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds & Log(Odds)\n",
    "\n",
    "Odds are not the same as probabilities.\n",
    "\n",
    "Odds are the ratio of something happening to something not happening.\n",
    "\n",
    "e.g - ratio of my team winning / my team not winning\n",
    "\n",
    "Probability is the ratio of something happening to everything that could happen.\n",
    "\n",
    "e.g - ratio of my team winning / my team winning and losing\n",
    "\n",
    "<img src='img/odds-1.png'/>\n",
    "\n",
    "In the above example,\n",
    "\n",
    "odds of winning = 5/3 = 1.7\n",
    "probability of winning = 5/8 = 0.625\n",
    "probability of losing = 3/8 = 0.375\n",
    "\n",
    "if we take the ratio of probability of winning to probability of losing i.e. (5/8) / (3/8) = 5/3 which is the same as odds.\n",
    "\n",
    "We can calculate the odds if we have the probability of winning. Let's call the probability of winning as 'p' then we can say\n",
    "\n",
    "odds = (p) / (1 - p)\n",
    "\n",
    "Why we need log(odds)\n",
    "\n",
    "consider how the odds of winning changes based on how worst the team goes, \n",
    "\n",
    "e.g. 1/4 = 0.25 , 1/8 = 0.124 , 1/32 = .03125 .. as the odds of winning goes from bad to worse , the value approaches 0. \n",
    "\n",
    "if the odds are against the team winning then the value will be between 0 and 1\n",
    "\n",
    "now let's consider how the odds of winning changes based on how the team performs better,\n",
    "\n",
    "e.g. 4/3 = 1.3 , 8/3 = 2.7 , 32/3 = 10.7\n",
    "\n",
    "If the odds are in favour of the team winning then the value will be between 1 and infinity\n",
    "\n",
    "<img src='img/odds-2.png'/>\n",
    "\n",
    "The magnitude of the odds of not in favour v/s in favour is not in the same scale and hence not symmetrical. Consider 1/6 and 6/1 in the below image\n",
    "\n",
    "<img src='img/odds-3.png'/>\n",
    "\n",
    "Now let's take log(odds) to make the whole thing symmetrical\n",
    "\n",
    "<img src='img/odds-4.png'/>\n",
    "\n",
    "Let's also quickly get introduced to the concept of 'odds ratio', consider the below dataset. \n",
    "The data represents 356 people\n",
    "- 29 of these people have cancer\n",
    "- 327 do not have cancer\n",
    "- 140 people have mutated gene\n",
    "- 216 people do not have mutated gene\n",
    "\n",
    "<img src='img/odds-6.png'/>\n",
    "\n",
    "We can use 'odds ratio' to determine if there is a relationship between mutated gene and cancer (if someone has mutated gene, are the odds higher that they have cancer ?)\n",
    "\n",
    "Given that the person have mutated gene, the odds that they have cancer are - 23/117\n",
    "Given that the person does not have mutated, gene the odds that they have cancer are - 6/210\n",
    "\n",
    "odds ratio = (23/117) / (6/210) = 6.88\n",
    "log(odds ratio) = log(6.88) = 1.93\n",
    "\n",
    "The odds ratio tells us that the odds are 6.88 times greater, the person with mutated gene also will have cancer\n",
    "\n",
    "Odds ratio and the log of the odd ratio are like R¬≤ , they indicate a relationship between two things. Just like R¬≤, the values corresponds to the effect size. i.e. larger value means that the mutated gene is a good predictor of cancer , smaller value means that the mutated gene is not a good predictor of cancer.\n",
    "\n",
    "Just like R¬≤, we need to know that if the relationship is statistically significant, people uses 3 ways to determine if the odds ratio (or the log(odds ratio)) is statistically significant\n",
    "\n",
    "- Fisher's Exact Test\n",
    "- Chi-Square Test\n",
    "- Wald Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit contd ...\n",
    "\n",
    "Going back to the previous discussion of calculating odds from the probabilities,\n",
    "\n",
    "<img src='img/odds-5.png'/>\n",
    "\n",
    "After the transformation the new y axis will change from -infinity to +infinity as the y axis is presenting logit (log of the odds). The new plot will also be a straight line as opposed to a curve.\n",
    "\n",
    "Now with logistic regression we can find the coefficients and errors. \n",
    "\n",
    "So far we were talking about a continous variable (weight) and it's relationship with obesity (categorical variable)\n",
    "\n",
    "Let's see how the concept applies to a categorical variable\n",
    "\n",
    "<img src='img/logistic-reg-7.png'/>\n",
    "\n",
    "This type of logistic regression is similar to how t-test is done using linear models\n",
    "\n",
    "<img src='img/logistic-reg-8.png'/>\n",
    "\n",
    "<img src='img/logistic-reg-9.png'/>\n",
    "\n",
    "We pair this equation with a design matrix to predict the size of the mouse given that it has normal or mutated version of the gene\n",
    "Design matrix : - first column corresponds to the values to be substituted for B1 and the second column corresponds to the value to be substituted for B2,\n",
    "\n",
    "<pre>\n",
    "| 1 0 |\n",
    "| 1 0 | \n",
    "| 1 0 |\n",
    "| 1 0 |\n",
    "| 1 1 |\n",
    "| 1 1 |\n",
    "| 1 1 |\n",
    "| 1 1 |\n",
    "</pre>\n",
    "\n",
    "<img src='img/logistic-reg-10.png'/>\n",
    "\n",
    "Now let's see how this concept of t-test is applied on logistic regression\n",
    "\n",
    "<img src='img/logistic-reg-11.png'/>\n",
    "\n",
    "<img src='img/logistic-reg-12.png'/>\n",
    "\n",
    "size = log(odds gene‚Çô‚Çí·µ£‚Çò‚Çê‚Çó) * B‚ÇÅ + (log(odds gene‚Çò·µ§‚Çú‚Çê‚Çú‚Çëùíπ) - log(odds gene‚Çô‚Çí·µ£‚Çò‚Çê‚Çó)) * B‚ÇÇ\n",
    "\n",
    "size = log(odds gene‚Çô‚Çí·µ£‚Çò‚Çê‚Çó) * B‚ÇÅ + (log(odds gene‚Çò·µ§‚Çú‚Çê‚Çú‚Çëùíπ / odds gene‚Çô‚Çí·µ£‚Çò‚Çê‚Çó) * B‚ÇÇ\n",
    "\n",
    "The second term here is log of odds ratio. It tells us, on a log scale, how much having the mutated gene increases(or decreases) the odds of mouse being obese \n",
    "\n",
    "When we solve the above equation with values, that's what we get as coefficients when we do logistic regression\n",
    "\n",
    "In short, in terms of coefficients, logistic regression is the exact same as good old linear models except the coeffcients are in terms of log(odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best fit curve a.k.a finding the maximum likelihood\n",
    "\n",
    "<img src='img/logistic-reg-13.PNG'/>\n",
    "\n",
    "We transform the y axis from probability of obesity to the log(odds of obesity). We cannot use least square in the new plot to get the best fit line because the residuals (distance from the data points to the line) are also infinity.\n",
    "\n",
    "<img src='img/logistic-reg-14.PNG'/>\n",
    "\n",
    "Instead of least squares we use maximum likelihood.\n",
    "\n",
    "We project the original data points onto the candidate line. This gives each point a log odds value.\n",
    "\n",
    "<img src='img/logistic-reg-15.PNG'/>\n",
    "\n",
    "We can transofrm the candidate log(odds) to candidate probabilities by  using this formulae\n",
    "\n",
    "p = eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ / ( 1 + eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ)\n",
    "\n",
    "<img src='img/logistic-reg-16.PNG'/>\n",
    "\n",
    "Basically if we have probability as input, then we can convert them to log(odds) and if we have log(odds) as input we can transform back to probability. Let's figure out how we arrived at this formulae\n",
    "\n",
    "log(p/(1-p)) = log(odds)\n",
    "\n",
    "p / (1-p) = eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ\n",
    "\n",
    "p = (1-p) * eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ \n",
    "\n",
    "p = eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ - p * eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ\n",
    "\n",
    "p + p * eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ =  eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ\n",
    "\n",
    "p * (1 + eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ) = eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ\n",
    "\n",
    "p = eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ / ( 1 + eÀ°·µí·µç‚ÅΩ·µí·µà·µàÀ¢‚Åæ)\n",
    "\n",
    "Now, for each points, pick the value on the y-axis , find the p value , plot the squiggle line.\n",
    "\n",
    "Find the likelyhood of each mouse being obese looking at the probability value. \n",
    "\n",
    "Likelyhood for all of the obese mouse are the product of individual likelyhoods.\n",
    "\n",
    "<img src='img/logistic-reg-17.PNG'/>\n",
    "\n",
    "Although it's possible to calculate the likelihood as the product of the individual likelihoods, statisticians prefer to calculate the log of the likelihood instead. The squiggle that maximizes the likelihood is the same as the one that maximizes the log of the likelihood\n",
    "\n",
    "log(likelihood of data given the squiggle) = log(0.49) + log(0.9) + log(0.91) + log(0.91) + log(0.92) + log(1-0.9) + log(1-0.3) + log(1-0.01) + log(1-0.01) \n",
    "\n",
    "Note - finding log (a * b ) = log a + log b\n",
    "\n",
    "Now we keep repeating the process till we find the best(maximizes) likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Found best fit curve - but how do we know it's useful ?\n",
    "\n",
    "Like how R¬≤ provides a way to measure how useful the fit is, in Logistic Regression we can use log-likelihood (LL) as a measure.\n",
    "\n",
    "We can use LL(fit) for the fitted line just like how we did SS(fit) for the fitted line in linear regression.\n",
    "\n",
    "We have to find something similar to SS(mean) in linear regression which tells us the poorly fitted line.\n",
    "\n",
    "We can find the log of odds of obesity for all the mice without considering weights. In the above example log(5/4) = 0.22. We get a horizontal line parallel to x and passing through y at point 0.22\n",
    "\n",
    "We project the data onto this line, find the probability and plot the probability squiggle\n",
    "\n",
    "<img src='img/logistic-reg-18.png'/>\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "<img src='img/logistic-reg-19.png'/>\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "<img src='img/logistic-reg-20.png'/>\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "<img src='img/logistic-reg-21.png'/>\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "<img src='img/logistic-reg-22.png'/>\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Like in Linear Regression, even for Logistic Regression R¬≤ value falls between 0 and 1 (0 when fit is bad and 1 when fit is best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for classification algorithms\n",
    "\n",
    "In Regression algorithm we find the accuracy metrics using,\n",
    "- MSE\n",
    "- RMSE\n",
    "- MAE\n",
    "- MAPE\n",
    "- R¬≤\n",
    "\n",
    "In Classification algorithm we find the accuracy metrics using,\n",
    "- Confusion matrix\n",
    "- proba graphs\n",
    "- ROC AUC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "- <b>True Positives</b> - These are patients that got heart disease and was correctly identified by algorithm\n",
    "- <b>True Negatives</b> - These are patients that did not have heard disease and was correctly identified by algorithm\n",
    "- <b>False Negatives</b> - Patients has heart disease but algorithm said they didnt\n",
    "- <b>False Positives</b> - Patients do not have heart disease but algorithm said they do\n",
    "\n",
    "Associate \n",
    "True -> Model predicting correctly | False -> Model predicting incorrectly\n",
    "Positive -> With heart condition | Negative -> Without heart condition\n",
    "\n",
    "- True's are correct predictions \n",
    "- False's are incorrect predictions\n",
    "    - False Negative - is incorrect Negative prediction. i.e. actual condition is positive.\n",
    "    - False Positive - is incorrect Positive prediction. i.e actual condition is negative.\n",
    "\n",
    "<img src='img/confmatrix-1.png'/>\n",
    "\n",
    "We can run multiple models (e.g KNN , Random Forest), create the confusion matrix and then see which one performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Curve\n",
    "\n",
    "ROC - Receiver Operations Characteristics\n",
    "AUC - Area Under Curve\n",
    "\n",
    "In the graphs seen earlier for Logistic Regression the Y axis represents the probability values.\n",
    "\n",
    "<img src='img/roc-1.png'/>\n",
    "\n",
    "We can define a threshold line (say at y = 0.5) above which we shall classify all mice as obese and below which the mice will be classified as not obese. Now when a new observation comes it's easier to predict the class based on the probability. But, how do we find the perfect threshold forms the crux of the problem\n",
    "\n",
    "Consider the 8 new data (4 obese and 4 non obese mice) for which we know the classification already and we try to run it through the model using the threshold 0.5\n",
    "\n",
    "<img src='img/roc-2.png'/>\n",
    "\n",
    "As we run through the classification, we see that \n",
    "- 3 non obese mice is predicted correctly\n",
    "- 3 obese mice is predicted correctly\n",
    "- 1 non obese is predcited as obese\n",
    "- 1 obese is predicted as non-obese\n",
    "\n",
    "<img src='img/roc-3.png'/>\n",
    "\n",
    "We plot a confusion matrix with these numbers\n",
    "\n",
    "<img src='img/roc-4.png'/>\n",
    "\n",
    "Now it's clear that if we play around with the threshold value (any value between 0 and 1) we get a whole bunch of confusion matrices to choose from. Based on the problem statement in hand (e.g cancer detection - we dont want to miss catching any cases which means False Positives are acceptable) we have to find an optimum threshold. How do we find the best threshold ? That's where ROC curve comes in handy\n",
    "\n",
    "ROC Curve - is a graph between FPR (False Positive Rate) & TPR (True Positive Rate)\n",
    "\n",
    "TPR (Sensitivity) = (True Positives) / (True Positives + False Negatives)\n",
    "\n",
    "FPR (1 - Specificity) = (False Positives) / (False Positives + True Negatives)\n",
    "\n",
    "<img src='img/roc-5.png'/>\n",
    "\n",
    "Green diagonal line shows where the TPR = FPR. \n",
    "Like you see, we plot the ROC curve with changing thresholds and thereby computing new TPRs and FPRs. \n",
    "\n",
    "The top most point on the y axis shows that model correctly classified 75% of the obese samples and 100% of the non obese samples.\n",
    "\n",
    "ROC graph summarizes all of the confusion matrices each of the threshold produced.\n",
    "\n",
    "<b> AUC - Area Under the Curve </b>\n",
    "Higher the area under the curve better is the curve.\n",
    "\n",
    "If red ROC curve represents logistic regression and blue represents random forest, we can choose logistic regression.\n",
    "\n",
    "<img src='img/auc-1.png'/>\n",
    "\n",
    "Note : instead of using FPR people might use precision\n",
    "Precision = (True Positives) / (True Positives + False Positives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "\n",
    "When we have 2 values in the y variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi class classification\n",
    "\n",
    "We can use an approach called <b>one v/s rest classifier</b>\n",
    "\n",
    "Let's say the y variable has values like 1,2,3 then it's a multi class classification problem.. \n",
    "\n",
    "Approach is going to be as follows,\n",
    "\n",
    "- Take all the rows from the training data where y=1. Let's assume there are 'n' rows. Flag the new y value as 'yes'\n",
    "- Take 'n' rows from the remaining dataset which will have values other than y=1. Flag the y value for this set as 'no'\n",
    "- Fit the data into a model (say model1) \n",
    "- Repeat the above steps for y=2 and y=3\n",
    "- After the models are fitted for y=1,2,3 we have 3 models (model1, model2, model3) who can classify 1,2,3 respectively.\n",
    "- The test data will be passed to the predict method of all the 3 models. The model giving the highest probability for 'yes' will be chosen and the 'y' variable it's trained to predict for will be the predicted 'y'variable. An example is given below which will clarify this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0  9  7  9  1  0\n",
       "1  0  6  2  2  1\n",
       "2  3  0  7  8  0\n",
       "3  3  4  7  0  0\n",
       "4  5  8  2  2  1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.DataFrame(np.random.randint(0,10,(1000,5)))\n",
    "data[4]=np.random.randint(0,3,1000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (1000, 4))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=data[4]\n",
    "x=data.drop(4,axis=1)\n",
    "y.shape,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=data[4]\n",
    "x=data.drop(4,axis=1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(x,y)\n",
    "lr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4\n",
       "0    9  7  9  1  0\n",
       "1    0  6  2  2  1\n",
       "2    3  0  7  8  0\n",
       "3    3  4  7  0  0\n",
       "4    5  8  2  2  1\n",
       "..  .. .. .. .. ..\n",
       "995  1  0  0  8  0\n",
       "996  0  7  7  0  1\n",
       "997  1  3  9  4  0\n",
       "998  2  2  3  3  2\n",
       "999  9  8  7  9  2\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((361, 5), (639, 5))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d2=data[data[4]!=0].sample(d1.shape[0])\n",
    "d0_yes = data[data[4] == 0]\n",
    "d0_no = data[data[4] != 0]\n",
    "d0_yes.shape, d0_no.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((361, 5), (361, 5))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take equal number of samples for y != 0 as y = 0\n",
    "d0_no = data[data[4]!=0].sample(d0_yes.shape[0])\n",
    "d0_yes.shape, d0_no.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['yes' 'No'] (361, 5) (361, 5) (722, 5)\n",
      "1 ['yes' 'No'] (356, 5) (356, 5) (712, 5)\n",
      "2 ['yes' 'No'] (283, 5) (283, 5) (566, 5)\n"
     ]
    }
   ],
   "source": [
    "predict_values=[]\n",
    "test_dataset=pd.DataFrame(np.random.randint(10,100,(100,4)))\n",
    "for i in data[4].unique():\n",
    "    d1=data[data[4]==i]\n",
    "    d1[4]=\"yes\"\n",
    "    d2=data[data[4]!=i].sample(d1.shape[0])\n",
    "    d2[4]=\"No\"\n",
    "    \n",
    "    final_dataset=d1.append(d2)\n",
    "    print(i,final_dataset[4].unique(),d1.shape,d2.shape,final_dataset.shape)\n",
    "    y=final_dataset[4]\n",
    "    x=final_dataset.drop(4,axis=1)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr=LogisticRegression()\n",
    "    lr.fit(x,y)\n",
    "    \n",
    "    predict_values.append(lr.predict_proba(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55963671, 0.44036329],\n",
       "       [0.61556008, 0.38443992],\n",
       "       [0.52392412, 0.47607588],\n",
       "       [0.69508851, 0.30491149],\n",
       "       [0.53344318, 0.46655682],\n",
       "       [0.74343216, 0.25656784],\n",
       "       [0.50082308, 0.49917692],\n",
       "       [0.79203109, 0.20796891],\n",
       "       [0.29756276, 0.70243724],\n",
       "       [0.52210053, 0.47789947],\n",
       "       [0.83822052, 0.16177948],\n",
       "       [0.74770023, 0.25229977],\n",
       "       [0.63515325, 0.36484675],\n",
       "       [0.59271319, 0.40728681],\n",
       "       [0.69602325, 0.30397675],\n",
       "       [0.2734842 , 0.7265158 ],\n",
       "       [0.54148839, 0.45851161],\n",
       "       [0.59539536, 0.40460464],\n",
       "       [0.30954354, 0.69045646],\n",
       "       [0.35339299, 0.64660701],\n",
       "       [0.75785578, 0.24214422],\n",
       "       [0.64751566, 0.35248434],\n",
       "       [0.74369677, 0.25630323],\n",
       "       [0.25633662, 0.74366338],\n",
       "       [0.58213656, 0.41786344],\n",
       "       [0.48797576, 0.51202424],\n",
       "       [0.26126072, 0.73873928],\n",
       "       [0.62498349, 0.37501651],\n",
       "       [0.76820897, 0.23179103],\n",
       "       [0.6524232 , 0.3475768 ],\n",
       "       [0.63194312, 0.36805688],\n",
       "       [0.81906235, 0.18093765],\n",
       "       [0.56761951, 0.43238049],\n",
       "       [0.39605866, 0.60394134],\n",
       "       [0.75705108, 0.24294892],\n",
       "       [0.7492146 , 0.2507854 ],\n",
       "       [0.72907479, 0.27092521],\n",
       "       [0.35395612, 0.64604388],\n",
       "       [0.69513115, 0.30486885],\n",
       "       [0.40335445, 0.59664555],\n",
       "       [0.56351553, 0.43648447],\n",
       "       [0.43324613, 0.56675387],\n",
       "       [0.43117614, 0.56882386],\n",
       "       [0.53029408, 0.46970592],\n",
       "       [0.42955594, 0.57044406],\n",
       "       [0.61912423, 0.38087577],\n",
       "       [0.60504676, 0.39495324],\n",
       "       [0.57778902, 0.42221098],\n",
       "       [0.74144117, 0.25855883],\n",
       "       [0.4704794 , 0.5295206 ],\n",
       "       [0.60642429, 0.39357571],\n",
       "       [0.47486577, 0.52513423],\n",
       "       [0.67903952, 0.32096048],\n",
       "       [0.43473989, 0.56526011],\n",
       "       [0.56516447, 0.43483553],\n",
       "       [0.82307464, 0.17692536],\n",
       "       [0.39183268, 0.60816732],\n",
       "       [0.4919753 , 0.5080247 ],\n",
       "       [0.74794254, 0.25205746],\n",
       "       [0.76596735, 0.23403265],\n",
       "       [0.76282074, 0.23717926],\n",
       "       [0.7658128 , 0.2341872 ],\n",
       "       [0.60840243, 0.39159757],\n",
       "       [0.68473668, 0.31526332],\n",
       "       [0.40080815, 0.59919185],\n",
       "       [0.59987077, 0.40012923],\n",
       "       [0.75222914, 0.24777086],\n",
       "       [0.55754997, 0.44245003],\n",
       "       [0.60335207, 0.39664793],\n",
       "       [0.65068787, 0.34931213],\n",
       "       [0.41449309, 0.58550691],\n",
       "       [0.62857641, 0.37142359],\n",
       "       [0.52968496, 0.47031504],\n",
       "       [0.72372307, 0.27627693],\n",
       "       [0.34533713, 0.65466287],\n",
       "       [0.46289821, 0.53710179],\n",
       "       [0.47934894, 0.52065106],\n",
       "       [0.35757992, 0.64242008],\n",
       "       [0.31996506, 0.68003494],\n",
       "       [0.69283307, 0.30716693],\n",
       "       [0.76805124, 0.23194876],\n",
       "       [0.32756099, 0.67243901],\n",
       "       [0.56806007, 0.43193993],\n",
       "       [0.43770601, 0.56229399],\n",
       "       [0.68335703, 0.31664297],\n",
       "       [0.46956224, 0.53043776],\n",
       "       [0.66470377, 0.33529623],\n",
       "       [0.26043587, 0.73956413],\n",
       "       [0.57805656, 0.42194344],\n",
       "       [0.32492485, 0.67507515],\n",
       "       [0.71989425, 0.28010575],\n",
       "       [0.51183858, 0.48816142],\n",
       "       [0.47522158, 0.52477842],\n",
       "       [0.39037859, 0.60962141],\n",
       "       [0.65584584, 0.34415416],\n",
       "       [0.46874364, 0.53125636],\n",
       "       [0.72863316, 0.27136684],\n",
       "       [0.41935518, 0.58064482],\n",
       "       [0.76520587, 0.23479413],\n",
       "       [0.48242406, 0.51757594]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_values[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_values=np.array(predict_values)\n",
    "predict_values.shape\n",
    "\n",
    "# 3 model outputs \n",
    "# each model output has 100 rows and 2 columns \n",
    "# first column is probability of 'yes' and second column is probability of 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2,\n",
       "       2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 0, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1,\n",
       "       1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from each model output take the first column alone i.e. the probability of 'yes' and combine them\n",
    "coordinates=list(zip(predict_values[0][:,0],predict_values[1][:,0],predict_values[2][:,0]))\n",
    "coordinates=np.array(coordinates)\n",
    "# coordinates is holding the probability of 'yes' from all the 3 models. Pick the one with maximum probability\n",
    "coordinates.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44959109, 0.49980695, 0.55963671],\n",
       "       [0.18404755, 0.59092475, 0.61556008],\n",
       "       [0.34553304, 0.68027955, 0.52392412],\n",
       "       [0.44808413, 0.42729006, 0.69508851],\n",
       "       [0.52527382, 0.58941462, 0.53344318],\n",
       "       [0.29031824, 0.47997413, 0.74343216],\n",
       "       [0.36783539, 0.60843459, 0.50082308],\n",
       "       [0.55776481, 0.40807196, 0.79203109],\n",
       "       [0.24945401, 0.82279227, 0.29756276],\n",
       "       [0.27042283, 0.67852263, 0.52210053],\n",
       "       [0.42972314, 0.32045653, 0.83822052],\n",
       "       [0.56426078, 0.42738565, 0.74770023],\n",
       "       [0.23364252, 0.55599025, 0.63515325],\n",
       "       [0.52764964, 0.52293778, 0.59271319],\n",
       "       [0.28156714, 0.47132578, 0.69602325],\n",
       "       [0.11781248, 0.8352843 , 0.2734842 ],\n",
       "       [0.50296376, 0.5895033 , 0.54148839],\n",
       "       [0.49303204, 0.59049003, 0.59539536],\n",
       "       [0.12147025, 0.7818295 , 0.30954354],\n",
       "       [0.14987127, 0.75540597, 0.35339299],\n",
       "       [0.25723742, 0.42722111, 0.75785578],\n",
       "       [0.20578771, 0.59643767, 0.64751566],\n",
       "       [0.42586833, 0.48624644, 0.74369677],\n",
       "       [0.14571324, 0.82472825, 0.25633662],\n",
       "       [0.30592895, 0.61126474, 0.58213656],\n",
       "       [0.28095348, 0.61655405, 0.48797576],\n",
       "       [0.10754451, 0.82709462, 0.26126072],\n",
       "       [0.14848969, 0.66717265, 0.62498349],\n",
       "       [0.34547561, 0.47531201, 0.76820897],\n",
       "       [0.21426256, 0.52162003, 0.6524232 ],\n",
       "       [0.27206209, 0.51357506, 0.63194312],\n",
       "       [0.45642487, 0.30503834, 0.81906235],\n",
       "       [0.41125225, 0.52292648, 0.56761951],\n",
       "       [0.35773467, 0.7403894 , 0.39605866],\n",
       "       [0.20123781, 0.4912661 , 0.75705108],\n",
       "       [0.28594803, 0.46524679, 0.7492146 ],\n",
       "       [0.36603804, 0.42462142, 0.72907479],\n",
       "       [0.30243294, 0.78990876, 0.35395612],\n",
       "       [0.13295309, 0.52651687, 0.69513115],\n",
       "       [0.32283076, 0.7610218 , 0.40335445],\n",
       "       [0.28068116, 0.60058016, 0.56351553],\n",
       "       [0.1851782 , 0.73575649, 0.43324613],\n",
       "       [0.34711401, 0.72937238, 0.43117614],\n",
       "       [0.19181193, 0.69253471, 0.53029408],\n",
       "       [0.12474627, 0.74867037, 0.42955594],\n",
       "       [0.13972552, 0.60169315, 0.61912423],\n",
       "       [0.20332594, 0.60981587, 0.60504676],\n",
       "       [0.4190789 , 0.56623756, 0.57778902],\n",
       "       [0.5265448 , 0.41429191, 0.74144117],\n",
       "       [0.22799586, 0.68895942, 0.4704794 ],\n",
       "       [0.15681296, 0.6632331 , 0.60642429],\n",
       "       [0.22864186, 0.69085791, 0.47486577],\n",
       "       [0.39701904, 0.41685758, 0.67903952],\n",
       "       [0.45899251, 0.69507048, 0.43473989],\n",
       "       [0.28587235, 0.57863981, 0.56516447],\n",
       "       [0.32210921, 0.34963799, 0.82307464],\n",
       "       [0.0877049 , 0.75979246, 0.39183268],\n",
       "       [0.30376785, 0.69599317, 0.4919753 ],\n",
       "       [0.4973295 , 0.36550483, 0.74794254],\n",
       "       [0.40768065, 0.46167051, 0.76596735],\n",
       "       [0.48596828, 0.39853351, 0.76282074],\n",
       "       [0.38724432, 0.46064134, 0.7658128 ],\n",
       "       [0.51810336, 0.50369944, 0.60840243],\n",
       "       [0.35960447, 0.55747311, 0.68473668],\n",
       "       [0.35738883, 0.68907615, 0.40080815],\n",
       "       [0.40460548, 0.47408576, 0.59987077],\n",
       "       [0.32518391, 0.47680468, 0.75222914],\n",
       "       [0.67440139, 0.54465314, 0.55754997],\n",
       "       [0.51183682, 0.47202394, 0.60335207],\n",
       "       [0.25977667, 0.59810688, 0.65068787],\n",
       "       [0.18976534, 0.72068005, 0.41449309],\n",
       "       [0.19277687, 0.6644557 , 0.62857641],\n",
       "       [0.43428335, 0.53464972, 0.52968496],\n",
       "       [0.37487819, 0.40371955, 0.72372307],\n",
       "       [0.20617337, 0.74396351, 0.34533713],\n",
       "       [0.23285131, 0.6653658 , 0.46289821],\n",
       "       [0.45710133, 0.62653794, 0.47934894],\n",
       "       [0.19865852, 0.77562656, 0.35757992],\n",
       "       [0.10762159, 0.78411242, 0.31996506],\n",
       "       [0.27201644, 0.54275026, 0.69283307],\n",
       "       [0.27395213, 0.43404837, 0.76805124],\n",
       "       [0.14811638, 0.78501923, 0.32756099],\n",
       "       [0.32679664, 0.65383769, 0.56806007],\n",
       "       [0.21377674, 0.75274111, 0.43770601],\n",
       "       [0.56555068, 0.46854863, 0.68335703],\n",
       "       [0.37618536, 0.61451351, 0.46956224],\n",
       "       [0.37775393, 0.52125263, 0.66470377],\n",
       "       [0.23188091, 0.83661709, 0.26043587],\n",
       "       [0.24621746, 0.68585141, 0.57805656],\n",
       "       [0.22789831, 0.7794021 , 0.32492485],\n",
       "       [0.18414964, 0.55413059, 0.71989425],\n",
       "       [0.23436318, 0.64442603, 0.51183858],\n",
       "       [0.23844706, 0.7082758 , 0.47522158],\n",
       "       [0.11921936, 0.77573794, 0.39037859],\n",
       "       [0.27128453, 0.5423421 , 0.65584584],\n",
       "       [0.33404776, 0.66597815, 0.46874364],\n",
       "       [0.40526407, 0.45129726, 0.72863316],\n",
       "       [0.19309856, 0.7616179 , 0.41935518],\n",
       "       [0.45482842, 0.39304212, 0.76520587],\n",
       "       [0.39548698, 0.61758702, 0.48242406]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44959109, 0.55040891],\n",
       "       [0.18404755, 0.81595245],\n",
       "       [0.34553304, 0.65446696],\n",
       "       [0.44808413, 0.55191587],\n",
       "       [0.52527382, 0.47472618],\n",
       "       [0.29031824, 0.70968176],\n",
       "       [0.36783539, 0.63216461],\n",
       "       [0.55776481, 0.44223519],\n",
       "       [0.24945401, 0.75054599],\n",
       "       [0.27042283, 0.72957717],\n",
       "       [0.42972314, 0.57027686],\n",
       "       [0.56426078, 0.43573922],\n",
       "       [0.23364252, 0.76635748],\n",
       "       [0.52764964, 0.47235036],\n",
       "       [0.28156714, 0.71843286],\n",
       "       [0.11781248, 0.88218752],\n",
       "       [0.50296376, 0.49703624],\n",
       "       [0.49303204, 0.50696796],\n",
       "       [0.12147025, 0.87852975],\n",
       "       [0.14987127, 0.85012873],\n",
       "       [0.25723742, 0.74276258],\n",
       "       [0.20578771, 0.79421229],\n",
       "       [0.42586833, 0.57413167],\n",
       "       [0.14571324, 0.85428676],\n",
       "       [0.30592895, 0.69407105],\n",
       "       [0.28095348, 0.71904652],\n",
       "       [0.10754451, 0.89245549],\n",
       "       [0.14848969, 0.85151031],\n",
       "       [0.34547561, 0.65452439],\n",
       "       [0.21426256, 0.78573744],\n",
       "       [0.27206209, 0.72793791],\n",
       "       [0.45642487, 0.54357513],\n",
       "       [0.41125225, 0.58874775],\n",
       "       [0.35773467, 0.64226533],\n",
       "       [0.20123781, 0.79876219],\n",
       "       [0.28594803, 0.71405197],\n",
       "       [0.36603804, 0.63396196],\n",
       "       [0.30243294, 0.69756706],\n",
       "       [0.13295309, 0.86704691],\n",
       "       [0.32283076, 0.67716924],\n",
       "       [0.28068116, 0.71931884],\n",
       "       [0.1851782 , 0.8148218 ],\n",
       "       [0.34711401, 0.65288599],\n",
       "       [0.19181193, 0.80818807],\n",
       "       [0.12474627, 0.87525373],\n",
       "       [0.13972552, 0.86027448],\n",
       "       [0.20332594, 0.79667406],\n",
       "       [0.4190789 , 0.5809211 ],\n",
       "       [0.5265448 , 0.4734552 ],\n",
       "       [0.22799586, 0.77200414],\n",
       "       [0.15681296, 0.84318704],\n",
       "       [0.22864186, 0.77135814],\n",
       "       [0.39701904, 0.60298096],\n",
       "       [0.45899251, 0.54100749],\n",
       "       [0.28587235, 0.71412765],\n",
       "       [0.32210921, 0.67789079],\n",
       "       [0.0877049 , 0.9122951 ],\n",
       "       [0.30376785, 0.69623215],\n",
       "       [0.4973295 , 0.5026705 ],\n",
       "       [0.40768065, 0.59231935],\n",
       "       [0.48596828, 0.51403172],\n",
       "       [0.38724432, 0.61275568],\n",
       "       [0.51810336, 0.48189664],\n",
       "       [0.35960447, 0.64039553],\n",
       "       [0.35738883, 0.64261117],\n",
       "       [0.40460548, 0.59539452],\n",
       "       [0.32518391, 0.67481609],\n",
       "       [0.67440139, 0.32559861],\n",
       "       [0.51183682, 0.48816318],\n",
       "       [0.25977667, 0.74022333],\n",
       "       [0.18976534, 0.81023466],\n",
       "       [0.19277687, 0.80722313],\n",
       "       [0.43428335, 0.56571665],\n",
       "       [0.37487819, 0.62512181],\n",
       "       [0.20617337, 0.79382663],\n",
       "       [0.23285131, 0.76714869],\n",
       "       [0.45710133, 0.54289867],\n",
       "       [0.19865852, 0.80134148],\n",
       "       [0.10762159, 0.89237841],\n",
       "       [0.27201644, 0.72798356],\n",
       "       [0.27395213, 0.72604787],\n",
       "       [0.14811638, 0.85188362],\n",
       "       [0.32679664, 0.67320336],\n",
       "       [0.21377674, 0.78622326],\n",
       "       [0.56555068, 0.43444932],\n",
       "       [0.37618536, 0.62381464],\n",
       "       [0.37775393, 0.62224607],\n",
       "       [0.23188091, 0.76811909],\n",
       "       [0.24621746, 0.75378254],\n",
       "       [0.22789831, 0.77210169],\n",
       "       [0.18414964, 0.81585036],\n",
       "       [0.23436318, 0.76563682],\n",
       "       [0.23844706, 0.76155294],\n",
       "       [0.11921936, 0.88078064],\n",
       "       [0.27128453, 0.72871547],\n",
       "       [0.33404776, 0.66595224],\n",
       "       [0.40526407, 0.59473593],\n",
       "       [0.19309856, 0.80690144],\n",
       "       [0.45482842, 0.54517158],\n",
       "       [0.39548698, 0.60451302]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we fitted data into 3 models (nunique of y variable is 3). Data fitted,\n",
    "\tFor model1 is y = 0 , y != 0 (equal number of rows as in y = 0)\n",
    "\tFor model2 is y = 1 , y != 1 (equal number of rows as in y = 1)\n",
    "\tFor model3 is y = 2 , y != 2 (equal number of rows as in y = 2)\n",
    "- Each model does prediction on the same test data set.\n",
    "- predict_values will have the probabilities of 'yes' and 'no' for each record in the test data set. \n",
    "- We pick all the 'yes' probabilities. Find the argument of the maximum probability. Let's imagine the argmax for a given row is 1 , this mean that the model2 gave the best result and hence the predicted y value for that row is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   44.45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 14 Apr 2020</td> <th>  Prob (F-statistic):</th>          <td>2.03e-23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:50:59</td>     <th>  Log-Likelihood:    </th>          <td> -244.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th>          <td>   499.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    95</td>      <th>  BIC:               </th>          <td>   512.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    0.1092</td> <td>    0.108</td> <td>    1.011</td> <td> 0.315</td> <td>   -0.105</td> <td>    0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>    0.3452</td> <td>    0.090</td> <td>    3.837</td> <td> 0.000</td> <td>    0.167</td> <td>    0.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>    0.2038</td> <td>    0.095</td> <td>    2.150</td> <td> 0.034</td> <td>    0.016</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>    0.0441</td> <td>    0.092</td> <td>    0.479</td> <td> 0.633</td> <td>   -0.139</td> <td>    0.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>    0.1833</td> <td>    0.089</td> <td>    2.070</td> <td> 0.041</td> <td>    0.008</td> <td>    0.359</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.219</td> <th>  Durbin-Watson:     </th> <td>   2.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.121</td> <th>  Jarque-Bera (JB):  </th> <td>   2.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.071</td> <th>  Prob(JB):          </th> <td>   0.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.274</td> <th>  Cond. No.          </th> <td>    4.54</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   0.701\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.685\n",
       "Method:                 Least Squares   F-statistic:                              44.45\n",
       "Date:                Tue, 14 Apr 2020   Prob (F-statistic):                    2.03e-23\n",
       "Time:                        21:50:59   Log-Likelihood:                         -244.49\n",
       "No. Observations:                 100   AIC:                                      499.0\n",
       "Df Residuals:                      95   BIC:                                      512.0\n",
       "Df Model:                           5                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.1092      0.108      1.011      0.315      -0.105       0.324\n",
       "x2             0.3452      0.090      3.837      0.000       0.167       0.524\n",
       "x3             0.2038      0.095      2.150      0.034       0.016       0.392\n",
       "x4             0.0441      0.092      0.479      0.633      -0.139       0.227\n",
       "x5             0.1833      0.089      2.070      0.041       0.008       0.359\n",
       "==============================================================================\n",
       "Omnibus:                        4.219   Durbin-Watson:                   2.170\n",
       "Prob(Omnibus):                  0.121   Jarque-Bera (JB):                2.278\n",
       "Skew:                           0.071   Prob(JB):                        0.320\n",
       "Kurtosis:                       2.274   Cond. No.                         4.54\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "\n",
    "x=pd.DataFrame(np.random.randint(0,10,(100,5)))\n",
    "x.columns=['x1','x2','x3','x4','x5']\n",
    "y=np.random.randint(0,10,100)\n",
    "### extra information as opposed to models\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as sfa\n",
    "model=sm.OLS(y,x)\n",
    "lm=model.fit()\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F Statistics\n",
    "### ANOVA\n",
    "\n",
    "We do analysis of variance when we want to figure out how much of variance exists between members within a group v/s members outside of the group. The expectation is members within a group shouldnt vary much but members across the group should vary. This concept will be important when we build decision trees.\n",
    "\n",
    "If there were only 2 groups to be compared then we could use t-test, but if there are more than 2 groups then ANOVA comes in handy.\n",
    "\n",
    "Also consider the 3 groups as 3 age group of people (let's say teenagers , mid age and old age) and saw we want to find the impact of a new medicine on these groups. So we give the dosage of the medicine to each of these people and see how the people characteristics are across and within each group\n",
    "\n",
    "Consider another example where we try to find the effect of the beverage people consume and their reaction time.\n",
    "\n",
    "<img src='img/anova-0.png'/>\n",
    "\n",
    "The null hypothesis considers there is no impact of drink on the response time\n",
    "<img src='img/anova-null.png'/>\n",
    "\n",
    "In the below situation, there is no much variation across groups. But within group we see a major variation. This means it's the people that's creating this variation and not the drink itself.\n",
    "<img src='img/anova-1.png'/>\n",
    "\n",
    "In the below situation, the variation is across the groups, so it's clear that there is an impact of the drink.\n",
    "<img src='img/anova-2.png'/>\n",
    "\n",
    "To generalize,\n",
    "<img src='img/anova-3.png'/>\n",
    "\n",
    "\n",
    "ANOVA - Analysis Of Variance\n",
    "\n",
    "Let's use the below dataset for our explanation\n",
    "\n",
    "<pre>\n",
    "<b><u>1     2     3<u/></b>\n",
    "3     5     5\n",
    "2     3     6\n",
    "1     4     7\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Of Squares Total (SST)\n",
    "Let's calculate the mean of each group,\n",
    "\n",
    "mean of <label style=\"color:DodgerBlue;\">group1</label> = 2\n",
    "\n",
    "mean of <label style=\"color:Tomato;\">group2</label> = 4\n",
    "\n",
    "mean of <label style=\"color:MediumSeaGreen;\">group3</label> = 6\n",
    "\n",
    "mean for all groups = 4\n",
    "\n",
    "Let's calculate the SST (Sum Of Squares Total) = <label style=\"color:DodgerBlue;\">(3-4)¬≤ + (2-4)¬≤ + (1-4)¬≤ </label> + <label style=\"color:Tomato;\">(5-4)¬≤ + (3-4)¬≤ + (4-4)¬≤ </label> + <label style=\"color:MediumSeaGreen;\">(5-4)¬≤ +(6-4)¬≤ + (7-4)¬≤</label>  =  <label style=\"color:DodgerBlue;\">14</label> + <label style=\"color:Tomato;\">2</label> + <label style=\"color:MediumSeaGreen;\">14</label> = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degrees of freedom\n",
    "In calculating the SST, degrees of freedom is (m * n) - 1  , where m is the number of groups and n is the number of people in each group. <b> Degrees of freedom </b> states that, given that we know the total mean is already known, it wouldn't make a difference even if we dont have the measurement of one of the observation. i.e. we can still figure out the SST, in the absence of 1 such value (given that the mean is known). This is called degrees of freedom. In the above example degrees of freedom is 8\n",
    "\n",
    "To calculate variance we divide 30 / 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to figure out how much of the SST is because of variation within the group\n",
    "\n",
    "#### Sum Of Squares Within (SSW)\n",
    "\n",
    "group1 = <label style=\"color:DodgerBlue;\">(3-2)¬≤ + (2-2)¬≤ + (1-2)¬≤ </label> = 2\n",
    "\n",
    "group2 = <label style=\"color:Tomato;\">(5-4)¬≤ + (3-4)¬≤ + (4-4)¬≤ </label> = 2\n",
    "\n",
    "group3 = <label style=\"color:MediumSeaGreen;\">(5-6)¬≤ +(6-6)¬≤ + (7-6)¬≤</label> = 2 \n",
    "\n",
    "SSW = 2 + 2 + 2 = 6\n",
    "\n",
    "Total variation was 30, 6 out of that 30 is comes from the variation within the group\n",
    "\n",
    "For finding SSW, let's figure out the degrees of freedom. For each group if there are n observations, and if the means is already known then we could still live with (n-1) records. \n",
    "\n",
    "So total degrees of freedom = m * (n-1) = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to figure out how much of the SST is because of variation between the groups\n",
    "\n",
    "#### Sum Of Squares Between (SSB)\n",
    "\n",
    "mean of <label style=\"color:DodgerBlue;\">group1</label> = 2\n",
    "\n",
    "mean of <label style=\"color:Tomato;\">group2</label> = 4\n",
    "\n",
    "mean of <label style=\"color:MediumSeaGreen;\">group3</label> = 6\n",
    "\n",
    "mean of mean = 4\n",
    "\n",
    "To find, how much of the variation is there between the groups, for each member we find the deviation from the \"mean of group\" TO \"mean of mean\". i.e.\n",
    "\n",
    "<label style=\"color:DodgerBlue;\">(2-4)¬≤ + (2-4)¬≤ + (2-4)¬≤</label> + <label style=\"color:Tomato;\">(4-4)¬≤ + (4-4)¬≤ + (4-4)¬≤</label> + <label style=\"color:MediumSeaGreen;\">(6-4)¬≤ + (6-4)¬≤ + (6-4)¬≤ </label> = 24\n",
    "\n",
    "Degrees of freedom in calculating SSB -> if we know the mean of 2 groups, then we could find the mean of the third group assuming we know the mean of mean already. This means if there are m groups, \n",
    "then degrees of freedom = (m-1)\n",
    "\n",
    "SST = 30 (Degrees of freedom (8) - (m * n) - 1)\n",
    "\n",
    "SSW = 6 (Degrees of freedom (6) - (m * (n-1))\n",
    "\n",
    "SSB = 24 (Degrees of freedom (2) - (m-1)\n",
    "\n",
    "Which means SST = SSW + SSB , Even the degrees of freedom is adding up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting the dots\n",
    "\n",
    "Let's consider the group1, group2, group3 as a result of an experiment. Let's consider we are giving 3 set of food to the people participating in an experiment. If the mean of the group1, group2 and group3 are all same , then that means there is no difference in the change of food. if the means is different then that means there is an impact of the food.\n",
    "\n",
    "null hypothesis = Food doesn't make any difference i.e. mean of group1 = mean of group2 = mean of group3\n",
    "alternative hypothesis = Food does make a difference i.e means are not the same\n",
    "\n",
    "Now when we do hypothesis testing, we consider F-Statistics\n",
    "\n",
    "F-Statistics = (SSB/(m-1)) / (SSW/m(n-1)) \n",
    "If numerator is much larger than the denominator, then that means the variation between the groups is high that means there is a difference in mean across groups. We can reject null hypothesis\n",
    "\n",
    "If denominator is larger, then the variation will be within the group. So we cannot reject null hypothesis\n",
    "\n",
    "We use CHAID (for this kind of statistics)\n",
    "\n",
    "applying F-statistics formulae from our example above,\n",
    "\n",
    "SSB = 24\n",
    "\n",
    "m-1 = 2\n",
    "\n",
    "SSW = 6\n",
    "\n",
    "m(n-1) = 6\n",
    "\n",
    "F-statistics = (24/2) / (6/6) = 12\n",
    "\n",
    "12 is a high number indicating the variation is higher between the groups. \n",
    "\n",
    "The numerator here is a CHI squared distribution and denominator is another CHI squared distribution. So F-Statistics is a ratio of two CHI squared distribution\n",
    "\n",
    "We look at F-Distribution table where we look for a given value of signicance score, and a given numerator degrees of freedom and a denominator degrees of freedom what value the table would give.\n",
    "\n",
    "F(2,6) for significance level 0.10 = 3.46 and 12 is a value way above that. So we reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skew and Kurtosis\n",
    "\n",
    "### Skewness\n",
    "\n",
    "It is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution.\n",
    "It differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n",
    "\n",
    "There are two types of Skewness: Positive and Negative\n",
    "\n",
    "<img src='img/skew-03.jpeg' />\n",
    "\n",
    "Positive Skewness means when the tail on the right side of the distribution is longer or fatter. The mean and median will be greater than the mode.\n",
    "\n",
    "Negative Skewness is when the tail of the left side of the distribution is longer or fatter than the tail on the right side. The mean and median will be less than the mode.\n",
    "\n",
    "Let us take a very common example of house prices. Suppose we have house values ranging from $100k to $1,000,000 with the average being $500,000.\n",
    "\n",
    "If the peak of the distribution was left of the average value, portraying a positive skewness in the distribution. It would mean that many houses were being sold for less than the average value, i.e. $500k. This could be for many reasons, but we are not going to interpret those reasons here.\n",
    "\n",
    "If the peak of the distributed data was right of the average value, that would mean a negative skew. This would mean that the houses were being sold for more than the average value.\n",
    "\n",
    "\n",
    "<img src='img/skewness.png' />\n",
    "\n",
    "\n",
    "### Kurtosis\n",
    "\n",
    "Kurtosis is all about the tails of the distribution ‚Äî not the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. It is actually the measure of outliers present in the distribution.\n",
    "\n",
    "High kurtosis in a data set is an indicator that data has heavy tails or outliers. If there is a high kurtosis, then, we need to investigate why do we have so many outliers. It indicates a lot of things, maybe wrong data entry or other things. Investigate!\n",
    "Low kurtosis in a data set is an indicator that data has light tails or lack of outliers. If we get low kurtosis(too good to be true), then also we need to investigate and trim the dataset of unwanted results.\n",
    "\n",
    "<img src='img/kurtosis-01.jpeg' />\n",
    "\n",
    "Mesokurtic: This distribution has kurtosis statistic similar to that of the normal distribution. It means that the extreme values of the distribution are similar to that of a normal distribution characteristic. This definition is used so that the standard normal distribution has a kurtosis of three.\n",
    "\n",
    "Leptokurtic (Kurtosis > 3): Distribution is longer, tails are fatter. Peak is higher and sharper than Mesokurtic, which means that data are heavy-tailed or profusion of outliers.\n",
    "Outliers stretch the horizontal axis of the histogram graph, which makes the bulk of the data appear in a narrow (‚Äúskinny‚Äù) vertical range, thereby giving the ‚Äúskinniness‚Äù of a leptokurtic distribution.\n",
    "\n",
    "Platykurtic: (Kurtosis < 3): Distribution is shorter, tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that data are light-tailed or lack of outliers.\n",
    "The reason for this is because the extreme values are less than that of the normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Read all the concepts below\n",
    "\n",
    "- F-Score\n",
    "- one way anova , two way anova\n",
    "- CHI squared distribution\n",
    "- F-distribution tables\n",
    "- significance values\n",
    "- skewness\n",
    "- kurbosis\n",
    "- stats model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa\n",
    "\n",
    "StatQuest - Youtube channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording at https://drive.google.com/open?id=1Ur-GFmT2vp17mtG8fo3Qrpd7WqFG7AAE"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
