{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Day 15 - Class </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved in solving a AI/ML problem\n",
    "\n",
    "- Problem definition\n",
    "- EDA\n",
    "- Preprocessing\n",
    "- Model building\n",
    "- Model validation\n",
    "- Feature engineering\n",
    "- Model deployment\n",
    "\n",
    "### Model building\n",
    "\n",
    "Machine learning models\n",
    "\n",
    "- Supervised\n",
    "- Unsupervised\n",
    "\n",
    "In supervised we have\n",
    "- Regression\n",
    "    - linear regression\n",
    "    - lasso regression\n",
    "    - ridge regression\n",
    "    - elastic net regression\n",
    "    - random forest regression\n",
    "    - decision tree regression\n",
    "    - SVM regression\n",
    "    \n",
    "- Classification\n",
    "    - Logistic regerssion\n",
    "    - Random forest classification\n",
    "    - decision tree classification\n",
    "    - bagging classification\n",
    "    - xg boost (xtreme gradient boosting)\n",
    "    - ADA Boost (adaptive boosting)\n",
    "    - gradient boosting\n",
    "    - catboost\n",
    "    \n",
    " Unsupervised \n",
    " - Clustering\n",
    "     - Kmeans\n",
    "     - Agglomerative\n",
    "     - K Prototype\n",
    "     - Kmodes\n",
    " - PCA (principal component analysis)\n",
    " - LDA (linear discriminant analysis)\n",
    " \n",
    " - KNN\n",
    " - Tensorflow, keras, OpenCV\n",
    " - ANN\n",
    " - CNN\n",
    " - NLP\n",
    "\n",
    "### Model Validation (Regression)\n",
    "- First thumbrule of a regression model is that 'y' should be a continous variable\n",
    "- Mean Absolute Error will validate how good the linear regression model is\n",
    "\n",
    "<b> Mean Absolute Error (MAE) </b>\n",
    "\n",
    "<img src='img/mae-01.png' />\n",
    "\n",
    "<b> Mean Squared Error (MSE) </b>\n",
    "<img src='img/mse-01.gif' />\n",
    "\n",
    "\n",
    "<b> Root Mean Squared Error (RMSE) </b>\n",
    "<img src='img/rmse-01.png' />\n",
    "\n",
    "\n",
    "<b> Mean Absolute Percentage Error (MAPE) </b>\n",
    "\n",
    "<img src='img/mape-01.jpeg' />\n",
    "\n",
    "Ypredicted - Yactual will give the loss \n",
    "\n",
    "Sum of all Ypredicted - Yactual will give the cost \n",
    "\n",
    "While calculating the cost (i.e. while summing) use the absolute value. \n",
    "\n",
    "MSE will penalize larger errors while neglect smaller errors for e.g,\n",
    "\n",
    "8^2 = 64\n",
    ".8 ^ 2 = 0.64 (see a small error of .8 is reduced further)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation (Classification)\n",
    "\n",
    "y variable here will be a categorical variable.\n",
    "\n",
    "We cannot use validation metrics such as MAE etc in this case. Instead we use confusion matrix.\n",
    "\n",
    "<img src='img/confusion-matrix-01.png' />\n",
    "\n",
    "Sum of TP,TN,FP,FN will be the total observations\n",
    "\n",
    "When we have 2 unique values in the y variable, then it's called binary classification more than 2 unique values means we are doing multiclass classification.\n",
    "\n",
    "There are other metrics in classification problems such as,\n",
    "- recall(sensitivity)\n",
    "- precesion(specificity)\n",
    "- f1-score\n",
    "- roc-auc curve\n",
    "\n",
    "We shall see them in detail when we cover classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "### Clustering - Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unsupervised algorithms there is a no 'y' variable. A good cluster is such that,\n",
    "\n",
    "1) The distance between all the points in the cluster to it's centroid is minimum\n",
    "\n",
    "2) The distance between different clusters formed should be maximum\n",
    "\n",
    "Different ways of finding distances are,\n",
    "\n",
    "1) Manhattan distance\n",
    "\n",
    "2) Minkowski distance\n",
    "\n",
    "3) Euclidean distance\n",
    "\n",
    "Clustering is one of the most common exploratory data analysis technique used to get an intuition about the structure of the data. It can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. In other words, we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as euclidean-based distance or correlation-based distance. \n",
    "\n",
    "Entropy is a measure of how homegenous or heterogenous the data is, within a cluster\n",
    "Lesser the entropy better the cluster. For homogenous data, entropy will be less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different clustering algorithms,\n",
    "\n",
    "1) K-Means\n",
    "\n",
    "2) K-Modes\n",
    "\n",
    "3) Agglomerative\n",
    "\n",
    "When we group the data by rows then its called CLUSTERING or SEGMENTATION\n",
    "\n",
    "When we group the data by columns then its called DIMENSIONALITY REDUCTION or FACTOR ANALYSIS\n",
    "\n",
    "Steps involved in implementing cluster analysis in real time\n",
    "\n",
    "1) Identify the clusters\n",
    "\n",
    "2) Study the behaviour of the clusters\n",
    "\n",
    "3) Design the strategies according to the cluster behaviour\n",
    "\n",
    "What algorithms to use when ?\n",
    "- K-Means - When all the data is continous variables\n",
    "- K-Modes - When all the data is class variables\n",
    "- K-Prototypes - When the data is a mix of continous and class variables\n",
    "\n",
    "\n",
    "In all clustering algorithms, if the distance between the data row and random point is same, then assign the point to the cluster with least(minimum) variance\n",
    "\n",
    "After forming clusters, target the members within the cluster. e.g. find all rows within cluster that has less mean value than the mean value of the cluster and start targetting such users to improve sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to choose the columns to be considered for K-Means ?\n",
    "\n",
    "- Pick continous variables \n",
    "- One way is to check how many unique values for a column\n",
    "- A better way is to check the variance of the fields. Higher the variance we need to consider such columns\n",
    "\n",
    "Once the cluster is formed plot it and see how it looks. Also look min/max/mean etc to see if the clusters are formed without any overlaps\n",
    "\n",
    "Steps\n",
    "\n",
    "1) Let's identify the number of clusters. There is a statistical way to find the number of clusters. For now,let's assume the number of clusters to be 3\n",
    "\n",
    "2) Pick 3 random rows from the data set ( 3 because we are going to create 3 clusters)\n",
    "\n",
    "3) Find the distance of all the rows with these 3 random points. Each row will be associated with 3 distances. Row will be assigned to the cluster with least distance. Once we do this for all the rows in the dataset, we would have assigned the row to either cluster1, cluster2 or cluster3\n",
    "\n",
    "4) Find the centroid of each cluster. Centroid will be calculated by finding the mean for each column i.e centroid = (mean of column1, mean of column2, mean of column3 ....). Since we have 3 clusters, there will be 3 centroids now\n",
    "\n",
    "5) Repeat step 2 - 4 by making the centroids as the new random points. After one such iteration the cluster would've changed\n",
    "\n",
    "6) In the above step, if the cluster doesn't change between 'n' th and (n+1) th iteration, then by now we would've formed the final clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps are very similar to KNN, just that in K-Means there is no 'y' variable but in KNN there is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical way of finding cluster is called K-Elbow\n",
    "\n",
    "What is K-Elbow ?\n",
    "\n",
    "- Divide the entire dataset into 1 cluster, find the variance\n",
    "- Divide the entire dataset into 2 clusters, find the variance of each cluster and sum it\n",
    "- Divide the entire dataset into 3 clusters, find the variance of each cluster and sum it \n",
    "- and so on...\n",
    "- After a particular point the sum of variance doesn't vary much. The point where we see a sharp drop in the sum of variance we can conside the number of cluster at that point as the number of clusters.\n",
    "\n",
    "If we plot the number of clusters in x axis and the sum of variance in y axis , then we can see a 'Elbow' like shape when the sum of variance drops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Modes\n",
    "\n",
    "k-modes is used for clustering categorical variables.\n",
    "\n",
    "1) Let's identify the number of clusters. There is a way to find the number of clusters, yet to be covered. The method name is 'Silhouette'\n",
    "\n",
    "2) Pick 3 random rows from the data set ( 3 because we are going to create 3 clusters)\n",
    "\n",
    "3) Form dissimilarity index(put value of 1 if not a match and 0 if its a match) for each rows and for each random points. i.e. Compare column1 value of the row with the column1 value of the random point and then put value of 1 and 0.  lower the sum of the elements in the dissimilarity index , closer the corresponding row to the respective random point.\n",
    "\n",
    "4) After 1 iteration we would've assigned the initial cluster. Find the centroid of each cluster. Centroid will be calculated by finding the mode for each column i.e centroid = (mode of column1,mode of column2, mode of column3 ....). Since we have 3 clusters, there will be 3 centroids now\n",
    "\n",
    "5) Repeat step 2 - 4 by making the centroids as the new random points. After one such iteration the cluster would've changed\n",
    "\n",
    "6) In the above step, if the cluster doesn't change between 'n' th and (n+1) th iteration, then by now we would've formed the final clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative\n",
    "\n",
    "Hierarchical clustering is one of the popular and easy to understand clustering technique. This clustering technique is divided into two types:\n",
    "\n",
    "- Agglomerative\n",
    "- Divisive\n",
    "\n",
    "Agglomerative Hierarchical clustering Technique: In this technique, initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\n",
    "\n",
    "The basic algorithm of Agglomerative is straight forward.\n",
    "\n",
    "    1) Compute the proximity matrix\n",
    "    \n",
    "    2) Let each data point be a cluster\n",
    "    \n",
    "    3) Repeat: Merge the two closest clusters and update the proximity matrix\n",
    "    \n",
    "    4) Until only a single cluster remains\n",
    "\n",
    "Key operation is the computation of the proximity of two clusters\n",
    "\n",
    "To understand better let’s see a pictorial representation of the Agglomerative Hierarchical clustering Technique. Lets say we have six data points {A,B,C,D,E,F}.\n",
    "\n",
    "    Step- 1: In the initial step, we calculate the proximity of individual points and consider all the six data points as individual clusters as shown in the image below.\n",
    "    \n",
    "    Step- 2: In step two, similar clusters are merged together and formed as a single cluster. Let’s consider B,C, and D,E are similar clusters that are merged in step two. Now, we’re left with four clusters which are A, BC, DE, F.\n",
    "    \n",
    "    Step- 3: We again calculate the proximity of new clusters and merge the similar clusters to form new clusters A, BC, DEF.\n",
    "\n",
    "    Step- 4: Calculate the proximity of the new clusters. The clusters DEF and BC are similar and merged together to form a new cluster. We’re now left with two clusters A, BCDEF.\n",
    "    \n",
    "    Step- 5: Finally, all the clusters are merged together and form a single cluster.\n",
    "\n",
    "<img src='img/agglo-01.png' />\n",
    "\n",
    "The Hierarchical clustering Technique can be visualized using a Dendrogram.\n",
    "\n",
    "A Dendrogram is a tree-like diagram that records the sequences of merges or splits.\n",
    "\n",
    "<img src='img/agglo-02.jpeg' />\n",
    "\n",
    "Divisive Hierarchical clustering Technique: Since the Divisive Hierarchical clustering Technique is not much used in the real world, I’ll give a brief of the Divisive Hierarchical clustering Technique.\n",
    "\n",
    "In simple words, we can say that the Divisive Hierarchical clustering is exactly the opposite of the Agglomerative Hierarchical clustering. In Divisive Hierarchical clustering, we consider all the data points as a single cluster and in each iteration, we separate the data points from the cluster which are not similar. Each data point which is separated is considered as an individual cluster. In the end, we’ll be left with n clusters.\n",
    "\n",
    "As we’re dividing the single clusters into n clusters, it is named as Divisive Hierarchical clustering.\n",
    "\n",
    "“HOW DO WE CALCULATE THE SIMILARITY BETWEEN TWO CLUSTERS???”\n",
    "\n",
    "Calculating the similarity between two clusters is important to merge or divide the clusters. There are certain approaches which are used to calculate the similarity between two clusters:\n",
    "\n",
    "- MIN (single-linkage algorithm)\n",
    "- MAX (complete linkage algorithm)\n",
    "- Group Average\n",
    "- Distance Between Centroids\n",
    "- Ward’s Method\n",
    "\n",
    "Read more at <a href='https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec'> this link </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a popular clustering method because of the heavy number of computations involved if there is a 100 row data set, then we form a 100*100 matrix. \n",
    "\n",
    "Positive is that the visualization created is very self explanatory\n",
    "\n",
    "5 types of link functions\n",
    "\n",
    "1) Single link (also known as Min link)\n",
    "\n",
    "2) Complete link\n",
    "\n",
    "3) Average Link\n",
    "\n",
    "4) Centroids\n",
    "\n",
    "5) Wards method (also known as Min Variance)\n",
    "\n",
    "For each data rows, r1,r2,r3....,rn find the distances between each row and every other row. This is where a n*n matrix will be formed\n",
    "\n",
    "<pre>\n",
    "      r1   r2   r3   r4\n",
    " r1   0    d1   d2   d3\n",
    " r2   d1   0    d4   d5  \n",
    " r3   d2   d4   0    d6          \n",
    " r4   d3   d5   d6   0         \n",
    "</pre>\n",
    "\n",
    "In this matrix we have distances between every row between every other row find the minimum distance , let's assumg d5 is minimum. Then in the next iteration we consider r4,r2 as combined data point\n",
    "\n",
    "<pre>\n",
    "        r1   r2   r3   r2r4\n",
    " r1     0    d1   d2   d3\n",
    " r2r4   d1   0    d4   d5  \n",
    " r3     d2   d4   0    d6          \n",
    "</pre> \n",
    "\n",
    "We have to repeat the above steps till we get to 1 cluster (not too clear on the intend behind this)when we find the distance between r1 and r2r4 i.e. r1 - r2r4\n",
    "1) if we consider the minimum value between (r1-r2) AND (r1-r4) then that's called Single link\n",
    "\n",
    "2) if we consider the maximum value between (r1-r2) AND (r1-r4) then that's called Complete link\n",
    "\n",
    "3) if we consider the average of (r1-r2) AND (r1-r4) then that's called Average Link\n",
    "\n",
    "4) if we find the centroid of (r1) and (r2 r4) and if we consider the distance between the centroids then it's called Centroids Link\n",
    "\n",
    "5) Ward's Link method \n",
    "\n",
    "- Find the mean of each cluster.\n",
    "\n",
    "- Calculate the distance between each object in a particular cluster, and that cluster’s mean.\n",
    "\n",
    "- Square the differences from Step 2.\n",
    "\n",
    "- Sum (add up) the squared values from Step 3.\n",
    "\n",
    "- Add up all the sums of squares from Step 4.\n",
    "\n",
    "#### Applications of clustering in real life\n",
    "\n",
    "- Customer segmentation\n",
    "\n",
    "Group a similar set of customers and target them to increase the sale. E.g. after forming clusters of customers, we can target customers who tend to purchase lesser than the average sales of the store\n",
    "\n",
    "- Stores segmentation\n",
    "\n",
    "Designing promotion offers specific to store/area etc\n",
    "\n",
    "- Products segmentation\n",
    "\n",
    "Cross selling and upselling of the products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Read Silhoutte method for finding 'k' for categorical variable clustering\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
