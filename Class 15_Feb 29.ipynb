{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Day 15 - Class </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved in solving a AI/ML problem\n",
    "\n",
    "- Problem definition\n",
    "- EDA\n",
    "- Preprocessing\n",
    "- Model building\n",
    "- Model validation\n",
    "- Feature engineering\n",
    "- Model deployment\n",
    "\n",
    "### Model building\n",
    "\n",
    "Machine learning models\n",
    "\n",
    "- Supervised\n",
    "- Unsupervised\n",
    "\n",
    "In supervised we have\n",
    "- Regression\n",
    "    - linear regression\n",
    "    - lasso regression\n",
    "    - ridge regression\n",
    "    - elastic net regression\n",
    "    - random forest regression\n",
    "    - decision tree regression\n",
    "    - SVM regression\n",
    "    \n",
    "- Classification\n",
    "    - Logistic regerssion\n",
    "    - Random forest classification\n",
    "    - decision tree classification\n",
    "    - bagging classification\n",
    "    - xg boost (xtreme gradient boosting)\n",
    "    - ADA Boost (adaptive boosting)\n",
    "    - gradient boosting\n",
    "    - catboost\n",
    "    \n",
    " Unsupervised \n",
    " - Clustering\n",
    "     - Kmeans\n",
    "     - Agglomerative\n",
    "     - K Prototype\n",
    "     - Kmodes\n",
    " - PCA (principal component analysis)\n",
    " - LDA (linear discriminant analysis)\n",
    " \n",
    " - KNN\n",
    " - Tensorflow, keras, OpenCV\n",
    " - ANN\n",
    " - CNN\n",
    " - NLP\n",
    "\n",
    "### Model Validation (Regression)\n",
    "- First thumbrule of a regression model is that 'y' should be a continous variable\n",
    "- Mean Absolute Error will validate how good the linear regression model is\n",
    "\n",
    "<b> Mean Absolute Error (MAE) </b>\n",
    "\n",
    "<img src='img/mae-01.png' />\n",
    "\n",
    "<b> Mean Squared Error (MSE) </b>\n",
    "<img src='img/mse-01.gif' />\n",
    "\n",
    "\n",
    "<b> Root Mean Squared Error (RMSE) </b>\n",
    "<img src='img/rmse-01.png' />\n",
    "\n",
    "\n",
    "<b> Mean Absolute Percentage Error (MAPE) </b>\n",
    "\n",
    "<img src='img/mape-01.jpeg' />\n",
    "\n",
    "Ypredicted - Yactual will give the loss \n",
    "\n",
    "Sum of all Ypredicted - Yactual will give the cost \n",
    "\n",
    "While calculating the cost (i.e. while summing) use the absolute value. \n",
    "\n",
    "MSE will penalize larger errors while neglect smaller errors for e.g,\n",
    "\n",
    "8^2 = 64\n",
    ".8 ^ 2 = 0.64 (see a small error of .8 is reduced further)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation (Classification)\n",
    "\n",
    "y variable here will be a categorical variable.\n",
    "\n",
    "We cannot use validation metrics such as MAE etc in this case. Instead we use confusion matrix.\n",
    "\n",
    "<img src='img/confusion-matrix-01.png' />\n",
    "\n",
    "Sum of TP,TN,FP,FN will be the total observations\n",
    "\n",
    "When we have 2 unique values in the y variable, then it's called binary classification more than 2 unique values means we are doing multiclass classification.\n",
    "\n",
    "There are other metrics in classification problems such as,\n",
    "- recall(sensitivity)\n",
    "- precesion(specificity)\n",
    "- f1-score\n",
    "- roc-auc curve\n",
    "\n",
    "We shall see them in detail when we cover classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "### Clustering - Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unsupervised algorithms there is a no 'y' variable. A good cluster is such that,\n",
    "\n",
    "1) The distance between all the points in the cluster to it's centroid is minimum\n",
    "\n",
    "2) The distance between different clusters formed should be maximum\n",
    "\n",
    "Different ways of finding distances are,\n",
    "\n",
    "1) Manhattan distance\n",
    "\n",
    "2) Minkowski distance\n",
    "\n",
    "3) Euclidean distance\n",
    "\n",
    "Entropy is a measure of how homegenous or heterogenous the data is, within a cluster\n",
    "Lesser the entropy better the cluster. For homogenous data, entropy will be less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different clustering algorithms,\n",
    "\n",
    "1) K-Means\n",
    "\n",
    "2) K-Modes\n",
    "\n",
    "3) Agglomerative\n",
    "\n",
    "When we group the data by rows then its called CLUSTERING or SEGMENTATION\n",
    "\n",
    "When we group the data by columns then its called DIMENSIONALITY REDUCTION or FACTOR ANALYSIS\n",
    "\n",
    "Steps involved in implementing cluster analysis in real time\n",
    "\n",
    "1) Identify the clusters\n",
    "\n",
    "2) Study the behaviour of the clusters\n",
    "\n",
    "3) Design the strategies according to the cluster behaviour\n",
    "\n",
    "What algorithms to use when ?\n",
    "- K-Means - When all the data is continous variables\n",
    "- K-Modes - When all the data is class variables\n",
    "- K-Prototypes - When the data is a mix of continous and class variables\n",
    "\n",
    "\n",
    "In all clustering algorithms, if the distance between the data row and random point is same, then assign the point to the cluster with least(minimum) variance\n",
    "\n",
    "After forming clusters, target the members within the cluster. e.g. find all rows within cluster that has less mean value than the mean value of the cluster and start targetting such users to improve sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to choose the columns to be considered for K-Means ?\n",
    "\n",
    "- Pick continous variables \n",
    "- One way is to check how many unique values for a column\n",
    "- A better way is to check the variance of the fields. Higher the variance we need to consider such columns\n",
    "\n",
    "Once the cluster is formed plot it and see how it looks. Also look min/max/mean etc to see if the clusters are formed without any overlaps\n",
    "\n",
    "Steps\n",
    "\n",
    "1) Let's identify the number of clusters. There is a statistical way to find the number of clusters. For now,let's assume the number of clusters to be 3\n",
    "\n",
    "2) Pick 3 random rows from the data set ( 3 because we are going to create 3 clusters)\n",
    "\n",
    "3) Find the distance of all the rows with these 3 random points. Each row will be associated with 3 distances. Row will be assigned to the cluster with least distance. Once we do this for all the rows in the dataset, we would have assigned the row to either cluster1, cluster2 or cluster3\n",
    "\n",
    "4) Find the centroid of each cluster. Centroid will be calculated by finding the mean for each column i.e centroid = (mean of column1, mean of column2, mean of column3 ....). Since we have 3 clusters, there will be 3 centroids now\n",
    "\n",
    "5) Repeat step 2 - 4 by making the centroids as the new random points. After one such iteration the cluster would've changed\n",
    "\n",
    "6) In the above step, if the cluster doesn't change between 'n' th and (n+1) th iteration, then by now we would've formed the final clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps are very similar to KNN, just that in K-Means there is no 'y' variable but in KNN there is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical way of finding cluster is called K-Elbow\n",
    "\n",
    "What is K-Elbow ?\n",
    "\n",
    "- Divide the entire dataset into 1 cluster, find the variance\n",
    "- Divide the entire dataset into 2 clusters, find the variance of each cluster and sum it\n",
    "- Divide the entire dataset into 3 clusters, find the variance of each cluster and sum it \n",
    "- and so on...\n",
    "- After a particular point the sum of variance doesn't vary much. The point where we see a sharp drop in the sum of variance we can conside the number of cluster at that point as the number of clusters.\n",
    "\n",
    "If we plot the number of clusters in x axis and the sum of variance in y axis , then we can see a 'Elbow' like shape when the sum of variance drops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps\n",
    "# 1) Let's identify the number of clusters. There is a way to find the number of clusters, yet to be covered. \n",
    "#    The method name is 'Silhouette'\n",
    "# 2) Pick 3 random rows from the data set ( 3 because we are going to create 3 clusters)\n",
    "# 3) Form dissimilarity index(put value of 1 if not a match and 0 if its a match) for each rows and for \n",
    "#    each random points. i.e. Compare column1 value of the row with the column1 value of the random point and \n",
    "#    then put value of 1 and 0.  lower the sum of the elements in the dissimilarity index , \n",
    "#    closer the corresponding row to the respective random point.\n",
    "# 4) After 1 iteration we would've assigned the initial cluster. Find the centroid of each cluster. \n",
    "#    Centroid will be calculated by finding the mode for each column i.e centroid = (mode of column1,\n",
    "#    mode of column2, mode of column3 ....). Since we have 3 clusters, there \n",
    "#    will be 3 centroids now\n",
    "# 5) Repeat step 2 - 4 by making the centroids as the new random points. After one such iteration the cluster\n",
    "#    would've changed\n",
    "# 6) In the above step, if the cluster doesn't change between 'n' th and (n+1) th iteration, then by now we \n",
    "#    would've formed the final clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not a popular clustering method because of the heavy number of computations involved\n",
    "# if there is a 100 row data set, then we form a 100*100 matrix. \n",
    "# Positive is that the visualization created is very self explanatory\n",
    "\n",
    "# 5 types of link functions\n",
    "# 1) Single link (also known as Min link)\n",
    "# 2) Complete link\n",
    "# 3) Average Link\n",
    "# 4) Centroids\n",
    "# 5) Wards method (also known as Min Variance)\n",
    "\n",
    "# For each data rows, r1,r2,r3....,rn find the distances between each row and every other row. This is where \n",
    "# a n*n matrix will be formed\n",
    "#      r1   r2   r3   r4\n",
    "# r1   0    d1   d2   d3\n",
    "# r2   d1   0    d4   d5  \n",
    "# r3   d2   d4   0    d6          \n",
    "# r4   d3   d5   d6   0         \n",
    "#\n",
    "# In this matrix we have distances between every row between every other row\n",
    "# find the minimum distance , let's assumg d5 is minimum. Then in the next iteration we consider r4,r2 as \n",
    "# combined data point\n",
    "#        r1   r2   r3   r2r4\n",
    "# r1     0    d1   d2   d3\n",
    "# r2r4   d1   0    d4   d5  \n",
    "# r3     d2   d4   0    d6          \n",
    "# \n",
    "# We have to repeat the above steps till we get to 1 cluster (not too clear on the intend behind this)\n",
    "# when we find the distance between r1 and r2r4 i.e. r1 - r2r4\n",
    "# 1) if we consider the minimum value between (r1-r2) AND (r1-r4) then that's called Single link\n",
    "# 2) if we consider the maximum value between (r1-r2) AND (r1-r4) then that's called Complete link\n",
    "# 3) if we consider the average of (r1-r2) AND (r1-r4) then that's called Average Link\n",
    "# 4) if we find the centroid of (r1) and (r2 r4) and if we consider the distance between the centroids then it's \n",
    "#    called Centroids Link\n",
    "# 5) Ward's Link method \n",
    "#    Find the mean of each cluster.\n",
    "#    Calculate the distance between each object in a particular cluster, and that clusterâ€™s mean.\n",
    "#    Square the differences from Step 2.\n",
    "#    Sum (add up) the squared values from Step 3.\n",
    "#    Add up all the sums of squares from Step 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendogram is the visual representation of how clusters are formed in an agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
