{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Day 25 - Class </h1>\n",
    "\n",
    "## CART Algorithms\n",
    "\n",
    "CART - Classification And Regression Trees\n",
    "\n",
    "Used for non linear data. \n",
    "\n",
    "Decision trees are the basis for the classification algorithm.\n",
    "\n",
    "- Classification\n",
    "    - Decision Tree Classification\n",
    "    - Random Forest Classification\n",
    "    - Bagging Classification\n",
    "\n",
    "- Regression\n",
    "    - Decision Tree Regression\n",
    "    - Random Forest Regression\n",
    "    - Bagging Regression\n",
    "\n",
    "- Ensemble Models\n",
    "    - Bagging\n",
    "    - Boosting\n",
    "        - ADA Boosting (Adaptive Boosting)\n",
    "        - Gradient Boosting (GB Boosting)\n",
    "        - XG Boosting  (Extreme Gradient)\n",
    "    - Stacking\n",
    "        - Blended Stacking Bootstrap Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification\n",
    "\n",
    "Binary classification problem - When your y variable has only 2 classes.\n",
    "\n",
    "Multi classification problem - When your y variable has more than 2 classes.\n",
    "\n",
    "Consider the below dataset, where we are trying to predict a persons EMI repayment capability based on his gender, existing car loan,existing home loan\n",
    "<img src='img/dt-2.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When can try to draw a tree as similar to the one in the right. let's try to interpret the tree.\n",
    "\n",
    "If we consider Gender as the root, \n",
    "- we have total 12 people \n",
    "    - out of which five people are in '0' category\n",
    "    - seven people are in '1' category. \n",
    "\n",
    "Considering second level as House Loan, \n",
    "- out of the five people who belongs to category '0' gender\n",
    "    - one doesnt have housing loan\n",
    "    - four has housing loan\n",
    "    \n",
    "- out of the seven people who belongs to category '1' gender\n",
    "    - four doesnt have housing loan\n",
    "    - three has housing loan\n",
    "    \n",
    "and so on .. I believe you got the drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum level of a tree, will be the number of features available in the dataset. <b> A fully grown tree </b> is a tree with all the columns considered.\n",
    "\n",
    "Once we have constructed a tree, let's consider a new row for which we have to predict the y value. Take each attribute of the new row and see which path will the new row take to reach a decision. Once you get to the leaf node you can say out of all the people with similar characteristics, 'm' is the probability that he has the capacity to pay the EMI and 'n' is the probability he doesnt not  have the capacity to pay the EMI. i.e. Majority voting applies. In regression problems, we take the mean of the people available.\n",
    "\n",
    "With fully grown decision trees, one of the major problem is 'Overfitting'. i.e. it works well on test data , but not in train data. i.e. Decision Tree is harder to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "How do we control/restrict the growth of a tree ?  Answer is <b> Pruning </b>\n",
    "\n",
    "How to Prune ?\n",
    "- Instead of considering all columns, we can pick important columns that's one way to Prune the data\n",
    "- On the decision nodes(nodes are called decision nodes), we can put conditions. i.e. create the node only if there are more than 'n' number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to decide which variables to be considered on what level of the tree ?\n",
    "Imagine a dataset where we have the following, \n",
    "- people who likes watching  movies .. let's imagine the distribution as 60 : 40\n",
    "- people who likes watching games .. let's imagine the distribition as 70 : 30 \n",
    "\n",
    "Which one would you consider for the root node ? Ofcourse people who likes watching movies, because the tree will be balanced better on both sides with that choice. \n",
    "\n",
    "The best split will be when , \n",
    "within the split, data has similar characteristics. \n",
    "Across splits, data should have different characteristics.\n",
    "\n",
    "We have to understand a few concepts to find the right split\n",
    "<b>\n",
    "<pre>\n",
    "- Entropy\n",
    "- GINI Index\n",
    "- Information Gain\n",
    "- CHAID\n",
    "</pre>\n",
    "</b>\n",
    "\n",
    "With these 4 ways , we can find the perfect column to start the tree.\n",
    "\n",
    "i.e. let's imagine x1,x2,x3,x4 and y as the dataset. We find information gain for each variables w.r.t to y\n",
    "\n",
    "find information gain of x1,y\n",
    "information gain of x2,y\n",
    "information gain of x3,y\n",
    "information gain of x4,y\n",
    "\n",
    "The field with the maximum information gain can be chosen as the first node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "\n",
    "Entropy is nothing but the measure of disorder. Entropy will be a value between 0 and 1. \n",
    "\n",
    "- 0 - means it's a homogeneous mixture (i.e. no disorder or in otherwords, the data is very pure)\n",
    "- 1 - means it's a heterogeneous mixture (i.e high disorder or in other words, the data is very impure)\n",
    "\n",
    "The Mathematical formula for Entropy is as follows\n",
    "\n",
    "<img src='img/entropy-03.png' />\n",
    "\n",
    "Let’s say we only have two classes , a positive class and a negative class. Therefore ‘i’ here could be either + or (-). So if we had a total of 100 data points in our dataset with 30 belonging to the positive class and 70 belonging to the negative class then ‘P+’ would be 3/10 and ‘P-’ would be 7/10. Pretty straightforward.\n",
    "\n",
    "If I was to calculate the entropy of my classes in this example using the formula above. Here’s what I would get.\n",
    "\n",
    "<img src='img/entropy-04.png' />\n",
    "\n",
    "The entropy here is approximately 0.88. This is considered a high entropy , a high level of disorder ( meaning low level of purity). Entropy is measured between 0 and 1.(Depending on the number of classes in your dataset, entropy can be greater than 1 but it means the same thing , a very high level of disorder. For the sake of simplicity, the examples in this blog will have entropy between 0 and 1).\n",
    "\n",
    "See the image below, when the data set has only +ves, or -ves, entropy is very low, when the data is a 50:50 mix, entropy is at maximum\n",
    "<img src='img/entropy-05.png' />\n",
    "\n",
    "#### Information Gain\n",
    "Now we know how to measure disorder. Next we need a metric to measure the reduction of this disorder in our target variable/class given additional information( features/independent variables) about it. This is where Information Gain comes in. Mathematically it can be written as:\n",
    "<img src='img/ig-02.png' />\n",
    "\n",
    "We simply subtract the entropy of Y given X from the entropy of just Y to calculate the reduction of uncertainty about Y given an additional piece of information X about Y. This is called Information Gain. The greater the reduction in this uncertainty, the more information is gained about Y from X.\n",
    "\n",
    "Let's take an example as shown below, here our target variable is Liability which can take on two values “Normal” and “High” and we only have one feature called Credit Rating which can take on values “Excellent”, “Good” and “Poor”. \n",
    "<img src='img/ig-03.png' />\n",
    "\n",
    "<img src='img/ig-04.png' />\n",
    "\n",
    "The entropy of our target variable is 1, at maximum disorder due to the even split between class label “Normal” and “High”. Our next step is to calculate the entropy of our target variable Liability given additional information about credit score.\n",
    "\n",
    "<img src='img/ig-05.png' />\n",
    "\n",
    "We got the entropy for our target variable given the feature Credit Rating. Now we can compute the Information Gain on Liability from Credit Rating to see how informative this feature is.\n",
    "\n",
    "<img src='img/ig-06.png' />\n",
    "\n",
    "Knowing the Credit Rating helped us reduce the uncertainty around our target variable, Liability! Isn’t that what a good feature is supposed to do? Provide us information about our target variable? Well that’s exactly how and why decision trees use entropy and information gain to determine which feature to split their nodes on to get closer to predicting the target variable with each split and also to determine when to stop splitting the tree! ( in addition to hyper-parameters like max depth of course). Let’s see this in action with another example using decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the important feature out of the available multiple features\n",
    "Consider an example where we are building a decision tree to predict whether a loan given to a person would result in a write-off or not. Our entire population consists of 30 instances. 16 belong to the write-off class and the other 14 belong to the non-write-off class.\n",
    "\n",
    "We have two features, namely \n",
    "- “Balance” that can take on two values -> “< 50K” or “>50K” and \n",
    "- “Residence” that can take on three values -> “OWN”, “RENT” or “OTHER”. \n",
    "\n",
    "I’m going to show you how a decision tree algorithm would decide what attribute to split on first and what feature provides more information, or reduces more uncertainty about our target variable out of the two using the concepts of Entropy and Information Gain.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Feature 1: Balance\n",
    "        </td>\n",
    "        <td>\n",
    "            Feature 2: Residence\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='img/ig-07.png'/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src='img/ig-08.png'/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Calculation for Feature - 1 (Balance)\n",
    "\n",
    "<img src='img/ig-09.png'/>\n",
    "\n",
    "Calculation for Feature - 2 (Residence)\n",
    "\n",
    "<img src='img/ig-10.png'/>\n",
    "\n",
    "The information gain from feature, Balance is almost 3 times more than the information gain from Residence! If you go back and take a look at the graphs you can see that the child nodes from splitting on Balance do seem purer than those of Residence. \n",
    "\n",
    "However the left most node for residence is also very pure but this is where the weighted averages come in play. Even though that node is very pure, it has the least amount of the total observations and a result contributes a small portion of it’s purity when we calculate the total entropy from splitting on Residence. This is important because we’re looking for overall informative power of a feature and we don’t want our results to be skewed by a rare value in a feature.\n",
    "\n",
    " A decision tree algorithm would use this result to make the first split on our data using Balance. From here on, the decision tree algorithm would use this process at every split to decide what feature it is going to split on next. In a real world scenario , with more than two features the first split is made on the most informative feature and then at every split the information gain for each additional feature needs to be recomputed because it would not be the same as the information gain from each feature by itself. The entropy and information gain would have to be calculated after one or more splits have already been made which would change the results. A decision tree would repeat this process as it grows deeper and deeper till either it reaches a pre-defined depth or no additional split can result in a higher information gain beyond a certain threshold which can also usually be specified as a hyper-parameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Entropy Calcuation\n",
    "\n",
    "Read the article <a href='https://www.saedsayad.com/decision_tree.htm'>here</a>\n",
    "\n",
    "a) Entropy using the frequency table of one attribute:\n",
    "\n",
    "<img src='img/entropy-1.png'/>\n",
    "\n",
    "Probability of playing golf = 9/14 = 0.64\n",
    "\n",
    "Probability of not playing golf = 5/14 = 0.36\n",
    "\n",
    "This is only with respect to one variable, we can also calculate entropy for x1,y | x2,y and so on\n",
    "\n",
    "b) Entropy using the frequency table of two attributes:\n",
    "\n",
    "<img src='img/entropy-2.png'/>\n",
    "\n",
    "Information Gain\n",
    "\n",
    "IG(X1) = E(Y) - E(Y,X1)\n",
    "<img src='img/ig-1.png'/> \n",
    "\n",
    "The column having the highest information gain can be selected as first level nodes in the tree. For every node of the tree Information Gain calculation is performed (we could see same field appearing again as well)\n",
    "\n",
    "#### Program for Entropy calculation\n",
    "\n",
    "First let's calculate entropy for y variable alone (i.e.  \"Entropy using the frequency table of one attribute\" - formulae 1  in the above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>23.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived     sex    who  adult_male     fare\n",
       "0           0    male    man        True   7.2500\n",
       "1           1  female  woman       False  71.2833\n",
       "2           1  female  woman       False   7.9250\n",
       "3           1  female  woman       False  53.1000\n",
       "4           0    male    man        True   8.0500\n",
       "..        ...     ...    ...         ...      ...\n",
       "886         0    male    man        True  13.0000\n",
       "887         1  female  woman       False  30.0000\n",
       "888         0  female  woman       False  23.4500\n",
       "889         1    male    man        True  30.0000\n",
       "890         0    male    man        True   7.7500\n",
       "\n",
       "[891 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df[['survived','sex','who','adult_male','fare']]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rani\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "survived      0\n",
       "sex           0\n",
       "who           0\n",
       "adult_male    0\n",
       "fare          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even we can use continous variables, but we convert them to binary\n",
    "x = []\n",
    "for i in dataset['fare']:\n",
    "    if i > dataset['fare'].mean() :\n",
    "        x.append(1)\n",
    "    else:\n",
    "        x.append(0)\n",
    "dataset['fare'] = x\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891,), (891, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can consider y variable as survived\n",
    "y = dataset['survived']\n",
    "x = dataset.drop('survived',axis=1)\n",
    "\n",
    "y.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6161616161616161, 0.3838383838383838)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilites = y.value_counts().values/y.value_counts().sum()\n",
    "p1 = probabilites[0]\n",
    "p2 = probabilites[1]\n",
    "p1,p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9607079018756469"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-p1 * math.log(p1,2)-p2 * math.log(p2,2)\n",
    "\n",
    "# Entropy for y = 0.9607079018756469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Now let's calculate Entropy for E(T,X) i.e. target variable and one of the X variable. Let's consider survived and gender </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>survived</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>female</td>\n",
       "      <td>81</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male</td>\n",
       "      <td>468</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "survived    0    1\n",
       "sex               \n",
       "female     81  233\n",
       "male      468  109"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#E(Survived,Gender)\n",
    "entropy_x1 = pd.crosstab(df['survived'],df['sex']).T\n",
    "entropy_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 81, 233],\n",
       "       [468, 109]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_x1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([314, 577], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of females, males\n",
    "entropy_x1.values.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total sum\n",
    "entropy_x1.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35241301907968575, 0.6475869809203143)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As per the formulae above, \n",
    "# E(Survived, Gender) = P(Female) * E(Female) + P(Male) * E(Male)\n",
    "# E(Survived, Gender) = P(Female) * E(81,233) + P(Male) * E(468,109)\n",
    "p_female, p_male = entropy_x1.values.sum(axis=1)/entropy_x1.values.sum()\n",
    "p_female, p_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81, 233], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_x1.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25796178343949044, 0.7420382165605095)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_fem_surv0, p_fem_surv1 = entropy_x1.values[0] / entropy_x1.values[0].sum()\n",
    "p_fem_surv0, p_fem_surv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8236550739295191"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_fem = -p_fem_surv0 * math.log(p_fem_surv0,2) - p_fem_surv1 * math.log(p_fem_surv1,2)\n",
    "entropy_fem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8110918544194108, 0.18890814558058924)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_male_surv0, p_male_surv1 = entropy_x1.values[1] / entropy_x1.values[1].sum()\n",
    "p_male_surv0, p_male_surv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6991817891208407"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_male = -p_male_surv0 * math.log(p_male_surv0,2) - p_male_surv1 * math.log(p_male_surv1,2)\n",
    "entropy_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7430477952150327"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_survived_gender = p_female * entropy_fem + p_male * entropy_male\n",
    "entropy_survived_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E(Survived, Gender) = 0.7430477952150327\n",
    "\n",
    "We did this for one X variable, we repease this for all X variables, and pick the X variable with the least \n",
    "entropy value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Now let's calculate Entropy for E(Survived,Fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>survived</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>464</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "survived    0    1\n",
       "fare              \n",
       "0         464  216\n",
       "1          85  126"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#E(Survived,Fare)\n",
    "entropy_x1 = pd.crosstab(dataset['survived'],dataset['fare']).T\n",
    "entropy_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7631874298540965, 0.23681257014590348)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_fare0, p_fare1 = entropy_x1.values.sum(axis=1)/entropy_x1.values.sum()\n",
    "p_fare0, p_fare1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([464, 216], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_x1.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 85, 126], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_x1.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fare0_surv0,p_fare0_surv1 = entropy_x1.values[0]/entropy_x1.values[0].sum()\n",
    "p_fare1_surv0,p_fare1_surv1 = entropy_x1.values[1]/entropy_x1.values[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42367834531837956"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_fare0 = -p_fare0_surv0 * (p_fare0_surv0 * math.log(p_fare0_surv0,2)) - p_fare0_surv1 * (p_fare0_surv1 * math.log(p_fare0_surv1,2))\n",
    "e_fare0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4781107068404824"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_fare1 = -p_fare1_surv0 * (p_fare1_surv0 * math.log(p_fare1_surv0,2)) - p_fare1_surv1 * (p_fare1_surv1 * math.log(p_fare1_surv1,2))\n",
    "e_fare1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4365686127495397"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_surv_fare = p_fare0 * e_fare0 + p_fare1 * e_fare1\n",
    "e_surv_fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GINI Index\n",
    "\n",
    "Decision trees recursively split features with regard to their target variable’s “purity”. The entire algorithm is designed to optimize each split on maximizing purity… What is purity? Purity can be thought of as how homogenized the groupings are. You will see in some example below what I mean:\n",
    "\n",
    "    If we have 4 red gumballs and 0 blue gumballs, that group of 4 is 100% pure, based on color as the target.\n",
    "\n",
    "    If we have 2 red and 2 blue, that group is 100% impure.\n",
    "\n",
    "    If we have 3 red and 1 blue, that group is either 75% or 81% pure, if we use Gini or Entropy respectively.\n",
    "    \n",
    "GINI index can be used only when we have to do binary splits(2 child nodes for a parent). With information gain we can have more than binary splits. Formula is\n",
    "\n",
    "<img src='img/gini-02.png'/>\n",
    "\n",
    "According to Wikipedia, the goal of GINI index is to “measure how often a randomly chosen element from the set would be incorrectly labeled”\n",
    "\n",
    "let’s go back to the gumball examples. If we decided to arbitrarily label all 4 gumballs as red, how often would one of the gumballs be incorrectly labeled?\n",
    "\n",
    "<b>4 red and 0 blue:</b>\n",
    "<img src='img/gini-03.png'/>\n",
    "\n",
    "A gini score of 0 is the most pure score possible.\n",
    "\n",
    "The impurity measurement is 0 because we would never incorrectly label any of the 4 red gumballs here. If we arbitrarily chose to label all the balls ‘blue’, then our index would still be 0, because we would always incorrectly label the gumballs.\n",
    "\n",
    "<b>2 red and 2 blue:</b>\n",
    "<img src='img/gini-04.png'/>\n",
    "\n",
    "The impurity measurement is 0.5 because we would incorrectly label gumballs wrong about half the time. Because this index is used in binary target variables (0,1), a gini index of 0.5 is the least pure score possible. Half is one type and half is the other. Dividing gini scores by 0.5 can help intuitively understand what the score represents. 0.5/0.5 = 1, meaning the grouping is as impure as possible (in a group with just 2 outcomes).\n",
    "\n",
    "Formulae = 1 - (p^2 + q^2)\n",
    "\n",
    "q is nothing but 1-p\n",
    "\n",
    "<img src='img/gini-1.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Red</th>\n",
       "      <th>Blue</th>\n",
       "      <th>Probability of Blue</th>\n",
       "      <th>Gini Score</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.060102</td>\n",
       "      <td>3.939898</td>\n",
       "      <td>0.984974</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.112515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.739830</td>\n",
       "      <td>2.260170</td>\n",
       "      <td>0.565043</td>\n",
       "      <td>0.491539</td>\n",
       "      <td>0.987759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.531620</td>\n",
       "      <td>0.468380</td>\n",
       "      <td>0.117095</td>\n",
       "      <td>0.206768</td>\n",
       "      <td>0.520953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.771062</td>\n",
       "      <td>2.228938</td>\n",
       "      <td>0.557235</td>\n",
       "      <td>0.493448</td>\n",
       "      <td>0.990527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.750494</td>\n",
       "      <td>2.249506</td>\n",
       "      <td>0.562377</td>\n",
       "      <td>0.492218</td>\n",
       "      <td>0.988744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>3.423997</td>\n",
       "      <td>0.576003</td>\n",
       "      <td>0.144001</td>\n",
       "      <td>0.246529</td>\n",
       "      <td>0.594622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>1.244460</td>\n",
       "      <td>2.755540</td>\n",
       "      <td>0.688885</td>\n",
       "      <td>0.428645</td>\n",
       "      <td>0.894456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>0.312357</td>\n",
       "      <td>3.687643</td>\n",
       "      <td>0.921911</td>\n",
       "      <td>0.143983</td>\n",
       "      <td>0.395411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>2.078522</td>\n",
       "      <td>1.921478</td>\n",
       "      <td>0.480370</td>\n",
       "      <td>0.499229</td>\n",
       "      <td>0.998888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>0.565218</td>\n",
       "      <td>3.434782</td>\n",
       "      <td>0.858695</td>\n",
       "      <td>0.242675</td>\n",
       "      <td>0.587645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Red      Blue  Probability of Blue  Gini Score   Entropy\n",
       "0     0.060102  3.939898             0.984974    0.029600  0.112515\n",
       "1     1.739830  2.260170             0.565043    0.491539  0.987759\n",
       "2     3.531620  0.468380             0.117095    0.206768  0.520953\n",
       "3     1.771062  2.228938             0.557235    0.493448  0.990527\n",
       "4     1.750494  2.249506             0.562377    0.492218  0.988744\n",
       "...        ...       ...                  ...         ...       ...\n",
       "9995  3.423997  0.576003             0.144001    0.246529  0.594622\n",
       "9996  1.244460  2.755540             0.688885    0.428645  0.894456\n",
       "9997  0.312357  3.687643             0.921911    0.143983  0.395411\n",
       "9998  2.078522  1.921478             0.480370    0.499229  0.998888\n",
       "9999  0.565218  3.434782             0.858695    0.242675  0.587645\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from math import log\n",
    "#Gini Function\n",
    "#a and b are the quantities of each class\n",
    "def gini(a,b):\n",
    "    a1 = (a/(a+b))**2\n",
    "    b1 = (b/(a+b))**2\n",
    "    return 1 - (a1 + b1)\n",
    "\n",
    "def entropy(base,a,b):\n",
    "    try:\n",
    "        var =  abs(((a)/(a+b)) * log(((a)/(a+b)),base)) - (((b)/(a+b)) * log(((b)/(a+b)),base))\n",
    "        return var\n",
    "    except (ValueError):\n",
    "        return 0#Blank lists\n",
    "\n",
    "#Blank lists\n",
    "gini_list = []\n",
    "ent_list = []\n",
    "blue_list = []\n",
    "red_list = []\n",
    "blue_prob_list = []#Looping Gini function on random blue and red float amounts\n",
    "for x in range (10000):\n",
    "    blue = random.uniform(0, 4)\n",
    "    red = abs(4-blue)\n",
    "    a_g = gini(red,blue)\n",
    "    a_e = entropy(2,red,blue)\n",
    "    b = blue/(blue+red)\n",
    "    gini_list.append(a_g)\n",
    "    ent_list.append(a_e)\n",
    "    blue_list.append(blue)\n",
    "    red_list.append(red)\n",
    "    blue_prob_list.append(b)#Dataframe of amount of blue, red, Probability of blue, and gini score\n",
    "\n",
    "df = pd.DataFrame({'Blue': blue_list, 'Red': red_list,'Gini Score': gini_list, 'Entropy': ent_list, 'Probability of Blue': blue_prob_list})\n",
    "df = df[['Red', 'Blue', 'Probability of Blue', 'Gini Score','Entropy']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Gini Curve')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dfbRdVX3u8e9j0gCGoCIJaggmF6IYgZvYIy/Sq9wK3kA6IB0XBYRWvAqlLfUFri0MKFaEAcItohVFsBZUFNBajBhlcFWsUsjlIEhAREKIJMIgqciLyIuB3/1jrYObnX3OnvucPffLWs9njIzsl3nWnmu/rGfNudaaUxGBmZnV14v6XQEzM+svB4GZWc05CMzMas5BYGZWcw4CM7OacxCYmdWcg8BqSdJFkv6+22XNhpF8HYFVkaQjgA8CuwNPAPcBlwGfiS5/6SW9EjgTOBjYFvglcCVwbkQ80c3XMsvBLQKrHEknAZ8AzgNeAewIHA/sB8zo8mttD9wIbAPsGxGzgAOBlwK7TGJ507tZP7MUDgKrFEkvAc4A/ioivhYRj0fh1og4KiKeLstdKunM8vb+kjZIOknSRkkPSnp3wzKfL9vCicDjwNERsQ4gItZHxPsj4nZJ8yVF4wZe0vWS3lvePkbSDZI+Lulh4KOSHpG0e0P52ZKelDSnvP8nkm4ry/2HpD27+BZaDTkIrGr2BbYCvtHh370CeAkwF3gPcKGklyX83QHA1yPiuQ5fr9HewFpgDkWIfR04suH5dwA/iIiNkt4AfB74C+DlwGeBFZK2msLrW805CKxqdgD+MyI2jz1Q7jU/Uu5Vv3mcv/sdcEZE/C4iVgK/AV6b8HovBx6cYp0fiIh/iojNEfEk8GVeGATvLB8DOBb4bESsiohnI+Iy4GlgnynWwWrMQWBV8ytgh8aumIh4U0S8tHxuvO/8rxrDA/gtxYHflNd75WQrW1rfdP97wDaS9pb0amAx8G/lc68GTiqD7RFJjwDzgFdNsQ5WYw4Cq5obKfaQD+3R6/1f4E8ljfdbGjtr6MUNj72iqcwLzmIqu5muomgVvBO4JiIeL59eD5wVES9t+PfiiPjKlNbCas1BYJUSEY8AHwE+LekwSdtKepGkxcDMDC95PrAdcFm5946kuZLOl7RnRGyiOJ30aEnTJP0v0s4m+jJwOHAUv+8WArgEOL5sLUjSTEnLJM3q6lpZrTgIrHIi4lyKs3n+FtgIPERxUPXvgP/o8ms9DLyJ4hjDKkmPA98FHgXWlMWOBT5E0Y30+pQ6RMQqitbEq4BvNzw+Wi7vU8Cvy9c4pjtrY3XlC8rMzGrOLQIzs5pzEJiZ1ZyDwMys5hwEZmY1N3QDXO2www4xf/78flfDzGyo3HLLLf8ZEbNbPTd0QTB//nxGR0f7XQ0zs6Ei6RfjPeeuITOzmnMQmJnVnIPAzKzmHARmZjXnIDAzqzkHgZlZzTkIzMxqzkFgZlZzDgIzs5pzEJiZ1VzWISYkLQU+AUwDPhcR5zQ9fwxwHsVUfgCfiojP5ayTWTfs+eHv8NjTzyaXX3fOsoy1MZuabEEgaRpwIXAgsAG4WdKKiPhpU9ErI+KEXPUwm4z5J3+rp8tzUFg/5WwR7AWsiYi1AJKuAA4FmoPArO+6veGf6us7GKyXcgbBXGB9w/0NwN4tyv1PSW8Gfg58MCLWNxeQdBxwHMDOO++coapWN5127fRaYzDst8v2XH7svn2sjVVdziBQi8ei6f43ga9ExNOSjgcuA/54iz+KuBi4GGBkZKR5GWZJjrrkRm649+F+V6NjN9z78PPBcPQ+O3Pm8j36XCOrmpxBsAGY13B/J+CBxgIR8auGu5cAH8tYH6upfnf7dNOXbrqfL910P+DuI+uenEFwM7BQ0gKKs4KOAN7ZWEDSKyPiwfLuIcBdGetjNVOlAGhlbP0cCDZV2YIgIjZLOgG4luL00c9HxJ2SzgBGI2IF8D5JhwCbgYeBY3LVx+ph77Ou46HHn+nLa49tkHsdQGOvt3DOTK47cf+evrZVgyKGq8t9ZGQkPFWlNTvt6tXPd5nk0q0979xB4RaCtSLplogYafmcg8CGXY4N6wWHL2b5krldX24ruc5gciBYIweBVVK3A2BQNpxVXS/rLweBVUo3N5SDvpHs1rpuPU387KyDu7IsG04OAquMbm0YBz0AmtV1va17HAQ29LqxIazKRtDvhU2Gg8CG2lQ3fFXd6Pl9sU44CGwoeUOXxu+TpZgoCDwxjQ2kqWzc9ttl+1pt3Nads4yFc2ZO+u+rfgW2tecWgQ2UqQwMJ+C+GgVAKwtO/tYWIzum8pXJ1eauIRsKU9kzrVMLIIXfS2vmriEbeJPdcK07Z5k3XC2sO2cZR+8zubk73FVUPw4C67uphICN78zle0z6PXIY1Iu7hqxvJjtSqAOgc1ff+ks+cOVtHf+djxtUh7uGbODMP/lbDoEeWr5k7qTeu3s2PuHWQQ04CKznJrNhWThnpkOgC9xVZK24a8h6ajIbFAdAHv4s6sVdQzYQvOEZLJN5b90yqCYHgfVEpxuQrafJIdADDgMDB4H1QKcbjgsOX+yx83to3TnL2G+X7Tv6G4dBtTgILKtONxjrzlnWsyki7fcuP3bfjlsHDoPqcBBYNpMJAesvh0E9OQgsC4fA8HIY1I+DwLrOITD8HAb14iCwrnIIVIfDoD4cBNY1DoHqWXfOMqYrvbzDYDg5CKwrOtkA+BqB4bLmbLcMqs5BYFPWyQ9/x1kzfI3AEHLLoNocBDYlnfzgF86ZyapTD8xYG8tpzdnL2HHWjOTyDoPh4SCwSevkh77fLtt7XPsKWHXqgR1dhewwGA5Zg0DSUkl3S1oj6eQJyh0mKSS1HBnPBk8nP/DpKq5ctWq4/Nh9WThnZnJ5h8HgyxYEkqYBFwIHAYuAIyUtalFuFvA+YFWuulh3dfrD7vRgow2+Tlt3DoPBlrNFsBewJiLWRsQzwBXAoS3KfRQ4F3gqY12sSzptCfjsoOrydQbVkTMI5gLrG+5vKB97nqQlwLyIuGaiBUk6TtKopNFNmzZ1v6aWZEGHp4i6JVB9nYbBbqeuzFQTm4qcQdDqZLPnp0OT9CLg48BJ7RYUERdHxEhEjMyePbuLVbRUR11yI53MZedTROujkzB46tnhmhGxLnIGwQZgXsP9nYAHGu7PAnYHrpe0DtgHWOEDxoPphnsfTi7r7qD66eQzdxfR4MkZBDcDCyUtkDQDOAJYMfZkRDwaETtExPyImA/cBBwSEZ6QeMB08sN1CNSXw2B4ZQuCiNgMnABcC9wFXBURd0o6Q9IhuV7XusshYJ1wGAwnRQxXn93IyEiMjrrR0AsOAZssf3cGj6RbIqJl17uvLLaW9j7ruuSy/iFbs06+E0ddcmPGmlgKB4G19NDjzySV63TSc6uP1HGJOjkRwfJwENgWUpv1wkNH2Pg6GWDQxwv6y0FgL9DJD/I+dwlZGz54PBwcBPY8H+CzHDr5rnRy9bp1j4PAADjt6tXJZR0C1qnU70zQ2XfRusNBYAB86ab7k8o5BGyyLjh8cVK51O+idY+DwJK7hDoZg96s2fIlc9l6Wtp8lz5e0FsOgprr5AfnGcZsqjoZjNBh0DsOghrr5EIedwlZt/his8HjIKix1At5HALWbanfKV9s1hsOgppKbXb7ymHLJfXgsbuI8nMQ1FAnPyxfOWy5LF8yt32hksMgLweBjctdQpabv2ODwUFQM6l7Vv6BWq+kftfcKsjHQVAjvl7ABlXa1QUOg1wcBDXRyWX7vl7Aeq2TAQw7mSvD0jgIasJDSNigS/3upc6VYekcBDXg4wI2LFJPV3YXUXc5CCrOzWgbJp2crnzg+dfnq0jNOAgqLrUZ7daADYrU7+I9G5/IXJP6cBBUmLuEbFj5lNLechBUVOpZQtttNS1zTcwmJ3XIand/Tp2DoKJSzxK6/SNLM9fEbHJSh6z2WURT5yCoIHcJWVW4i6g3HAQVkzp++9H77Jy5Jma95bmOJ89BUDGp47efuXyPzDUx647UVoHnOp48B0GFuEvIqspdRHllDQJJSyXdLWmNpJNbPH+8pNWSbpP0I0mLctanyq6+9ZdJ5VLPxDAbNP7m5pMtCCRNAy4EDgIWAUe22NB/OSL2iIjFwLnA+bnqU3UfuPK2pHKdTB5uNkhSB6Zzq6BzOVsEewFrImJtRDwDXAEc2lggIh5ruDsTiIz1qSx3CVldpH6Hdz3FYdCJnEEwF1jfcH9D+dgLSPprSfdStAje12pBko6TNCppdNOmTVkqW3W+cMyqYsdZM9qW2exdyo7kDIJWXXpbfDwRcWFE7AL8HXBaqwVFxMURMRIRI7Nnz+5yNYdbamvAF45ZVaw69cCkcu4iSpczCDYA8xru7wQ8MEH5K4DlGetTOanXDLhLyKom9Tu9wGGQJGcQ3AwslLRA0gzgCGBFYwFJCxvuLgPuyVifykm9ZsCsrtxDlCZbEETEZuAE4FrgLuCqiLhT0hmSDimLnSDpTkm3AScC78pVn6rxAWKrO7cKumd6zoVHxEpgZdNjpzfcfn/O1687DyNhVXfB4YvbnjrtVkF7vrJ4CKW2BjyMhFXd8iVbnIjYkg8cT8xBMGRSz492l5DVRep3PfXkijpyEAwZnx9tNjk+uWJ8DoIh4gPEZq15ULqpcRBUjEPA6uqCwxf3uwpDy0EwJFL2ZDw6o9WZDxxPnoNgCKROzp06OqNZVfnagsmZMAgk/aj8/3FJjzX8e1zSYxP9rXWPJ+c2S5cy54bPuXihCYMgIv6o/H9WRGzX8G9WRGzXmyrWW+qei48NmBVS59xIbWnXQXLXkKRpkl4laeexfzkrZoWUPZeFc2Zmr4fZMEm5qt4t7d9LCgJJfwM8BFwHfKv8d03GehnpB7WuO3H/vBUxGzKpV9X7wHEhtUXwfuC1EfH6cmrJPSJiz5wVszTuEjJrzb+NdKlBsB54NGdF7IVS9lSm+3xRsylzqyA9CNYC10s6RdKJY/9yVqzOUsdEWXO293jMJuJxiNKkDkN9f/lvRvnPMkoZE8VDTJulEe1Puqj7OESKGK4zakdGRmJ0dLTf1chmt1NX8tSz7T8T93+apUvp/tlx1ozk+ZCHkaRbImKk1XMTtggkXRARH5D0TVpPPH9Iiz+zKUgJAZ8uataZhXNmcs/GJyYsU+fTSSdsEUj6w4i4RdJbGh4e+wNFxA+y1q6FKrcIPLqoWT4pv6+tpyn5grRhM1GLoN3B4p0k/XVE/KDc6J8HXAZcCszpbjUthUPAbHJSfjspLfIqahcEfwusaLg/AxgB9geOz1SnWtrt1JXtC5lZdnU8nbRdEMyIiPUN938UEb+KiPsBd1R3kQ8Qm+Xn31Br7YLgZY13IuKEhruzu1+deko9o8HMeqNurYJ2QbBK0rHND0r6C+D/5amStVLl09rMesmtgi21u6Dsg8DVkt4J/Lh87A+BrYDlOStWF24NmPXedltN47Gnn52wzPyTv1Wb0Gg3H8HGiHgT8FFgXfnvjIjYNyIeyl+9aku9rN2tAbPuuv0jS/tdhYGSNNZQRHwvIv6p/Pe93JWqi5TL2t0aMMsjZZiWuhwr8JzFfZJ6uqhbA2Z5pM5ZUIcB6RwEfZJyuugFhy/uQU3M6ivlN1aHAekcBH2QMleqgOVL5uavjFmNpf7GTrt6deaa9FfWIJC0VNLdktZIOrnF8ydK+qmk2yV9V9Krc9ZnUKQMbnVfTc5WMOu3lDODvnTT/T2oSf9kCwJJ04ALgYOARcCRkhY1FbsVGCmnvfwacG6u+gyKXU+px8Ens6o58Pzr+12FbHK2CPYC1kTE2oh4BrgCOLSxQER8PyJ+W969CdgpY30GwuaEMa3qcu6y2aBI+c21G8Z6mOUMgrkUcx2P2VA+Np73AN9u9YSk4ySNShrdtGlTF6vYW24NmA23q2/9Zb+rkEXOIGg1tXrL/WFJR1OManpeq+cj4uKIGImIkdmzh3eII7cGzAZXym/vA1fe1oOa9F7OINgAzGu4vxPwQHMhSQcApwKHRMTTGevTVx5m2qwaqnisIGcQ3AwslLRA0gzgCF44twGSlgCfpQiBjRnr0nceZtps8NX1WEG2IIiIzcAJwLXAXcBVEXGnpDMkjc11fB6wLfBVSbdJWjHO4oaaWwNm1VK1q43bjT46JRGxEljZ9NjpDbcPyPn6g8KtAbPhse6cZW3HGKra1ca+sjizugxaZVY3VfptOwgGgFsDZoOlbr9JB0FGe374O/2ugplN0vRWJ8A3qcq1QQ6CjNrNgAT12/MwGxZrzm7/20y5NmgYOAgyWZDQf5iyx2Fmg60KrQIHQSYpOwopexxm1j8pLfYqtAocBBmkHBvYepqbA2bDIKXlntIDMMgcBBmkHBv42VkH96AmZjZVKS33YW8UOAi6zGcKmVVPSgt+mGcxcxB0mc8UMquelBb8MM9i5iDoopQ9Ah8bMBtOKb/cYZ2vwEHQRSl7BD42YDacUuYRH9b5ChwEXTKsewJmZg6CLknZE/CxAbPhlvIbHsbB6BwEZmY15yDogpQ9ALcGzKqhiq0CB4GZWc05CKbIrQGz+qlaq8BBYGZWcw6CKdj7rOvalnFrwKyajt5n57ZlhqVV4CCYgocef6bfVTCzPjlz+R79rkLXOAgmKSXp99tl+x7UxMz6ZcdZM9qWOfD86/NXZIocBBldfuy+/a6CmWW06tQD25a5Z+MTPajJ1DgIJmG3U1e2LeNpKM3qYeGcmW3LDHqrwEEwCU89234aCk9DaVYP1524f9syg94qcBB0KGWo6ZR+QzOzQeEg6FDKUNMp/YZmVh3DfoGZg6DLPPGMmQ2brEEgaamkuyWtkXRyi+ffLOnHkjZLOixnXbohJdE98YxZPQ1zqyBbEEiaBlwIHAQsAo6UtKip2P3AMcCXc9XDzMwmlrNFsBewJiLWRsQzwBXAoY0FImJdRNwOPJexHl3hweXMrJ1hbRXkDIK5wPqG+xvKxzom6ThJo5JGN23a1JXKmZlZIWcQtDpq2v4E/FZ/FHFxRIxExMjs2bOnWK3O7XqKWwNmliZlaJmUASt7KWcQbADmNdzfCXgg4+tls3lS8WVmdZQytMygDViZMwhuBhZKWiBpBnAEsCLj62Vx9a2/bFvGF5CZWaNh2yZkC4KI2AycAFwL3AVcFRF3SjpD0iEAkt4oaQPwduCzku7MVZ/J+sCVt7Ut4wvIzKxRyjZhkA4aT8+58IhYCaxseuz0hts3U3QZDa1hS34z643pGp5uZV9ZPIGUxHZrwMxaSRl4clBaBQ6CKfBwEmZWBQ6CcaTMOeDhJMxsIsNygZmDYBwpcw6YmVWBg6CFlNmEUmYlMjNLOaEkZZ6TnBwELaTMJpQyK5GZWcoJJSnznOTkIJiE7baa1u8qmNkQGfQTSxwETVIO3Nz+kaU9qImZVUXKiSX9PGjsIDAzqzkHQYOjLrmxbRmPMmpmk5Gy7djzw9/pQU225CBocMO9D/e7CmZWY489/WxfXtdBUEoZH9ynjJrZVKQcNE4Z8bjbHASllPHBfcqomU1FykHjlBGPu81BkChl1iEzs3YGccRiBwFpp22lzDpkZtZOygVmvT5o7CAwMxswvT5oXPsgSDlI7FNGzaybUrYpvZzgvvZBMGiTSJuZQW+3TbUPgnZ8kNjMchik8YdqHQQLfJDYzPok5VTSlG1UN9Q6CDz1jJkNsl5to2obBClJ64PEZpZTyjamF62C2gaBWwNmNgx6sa2qbRC049aAmfXCIFxpXMsg6OcEEGZmjVKuNM69zaplELTjU0bNrJem9/lM0toFQcrVej5l1Mx6ac3Z/T1oXLsg8JXEZjaMch40rlUQpEz44IPEZtYPFxy+uG+vnTUIJC2VdLekNZJObvH8VpKuLJ9fJWl+zvr0Y8IHM7MUy5fMbVsm10HjbEEgaRpwIXAQsAg4UtKipmLvAX4dEbsCHwc+lqs+KQZn5A8zq6N+TYebs0WwF7AmItZGxDPAFcChTWUOBS4rb38NeKukvm2P73O3kJn1Ub+mw80ZBHOB9Q33N5SPtSwTEZuBR4GXNy9I0nGSRiWNbtq0KVN1zczqKWcQtNqzbz7wnVKGiLg4IkYiYmT27NldqVwzHyQ2s0HQj21RziDYAMxruL8T8MB4ZSRNB14CPJyrQuP1v/WrX87MrJXxLmrNdbFrziC4GVgoaYGkGcARwIqmMiuAd5W3DwO+FxHZTpe97sT9t9joL5wzs2/9cmZmrVx+7L5bbPT322X7bBe7KuN2F0kHAxcA04DPR8RZks4ARiNihaStgS8CSyhaAkdExNqJljkyMhKjo6PZ6mxmVkWSbomIkVbPTc/5whGxEljZ9NjpDbefAt6esw5mZjaxWl1ZbGZmW3IQmJnVnIPAzKzmHARmZjXnIDAzqzkHgZlZzTkIzMxqzkFgZlZzDgIzs5rLOsREDpI2Ab/owqJ2AP6zC8sZFl7f6qrTuoLXd7JeHREth28euiDoFkmj4427UUVe3+qq07qC1zcHdw2ZmdWcg8DMrObqHAQX97sCPeb1ra46rSt4fbuutscIzMysUOcWgZmZ4SAwM6u9ygeBpKWS7pa0RtLJLZ7fStKV5fOrJM3vfS27I2FdT5T0U0m3S/qupFf3o57d0m59G8odJikkDfUphynrK+kd5Wd8p6Qv97qO3ZTwfd5Z0vcl3Vp+pw/uRz27QdLnJW2UdMc4z0vSJ8v34nZJb+hqBSKisv8o5kq+F/gvwAzgJ8CipjJ/BVxU3j4CuLLf9c64rv8deHF5+y+HdV1T17csNwv4d+AmYKTf9c78+S4EbgVeVt6f0+96Z17fi4G/LG8vAtb1u95TWN83A28A7hjn+YOBbwMC9gFWdfP1q94i2AtYExFrI+IZ4Arg0KYyhwKXlbe/BrxVknpYx25pu64R8f2I+G159yZgpx7XsZtSPluAjwLnAk/1snIZpKzvscCFEfFrgIjY2OM6dlPK+gawXXn7JcADPaxfV0XEvwMPT1DkUOALUbgJeKmkV3br9aseBHOB9Q33N5SPtSwTEZuBR4GX96R23ZWyro3eQ7GHMazarq+kJcC8iLimlxXLJOXzfQ3wGkk3SLpJ0tKe1a77Utb3H4CjJW0AVgJ/05uq9UWnv++OTO/WggZUqz375vNlU8oMg+T1kHQ0MAK8JWuN8ppwfSW9CPg4cEyvKpRZyuc7naJ7aH+K1t4PJe0eEY9krlsOKet7JHBpRPyjpH2BL5br+1z+6vVc1u1U1VsEG4B5Dfd3Ysvm4/NlJE2naGJO1EQbVCnriqQDgFOBQyLi6R7VLYd26zsL2B24XtI6in7VFUN8wDj1u/yNiPhdRNwH3E0RDMMoZX3fA1wFEBE3AltTDNBWRUm/78mqehDcDCyUtEDSDIqDwSuayqwA3lXePgz4XpRHZ4ZM23Utu0o+SxECw9x/DG3WNyIejYgdImJ+RMynOCZySESM9qe6U5byXb6a4oQAJO1A0VW0tqe17J6U9b0feCuApNdRBMGmntayd1YAf16ePbQP8GhEPNithVe6aygiNks6AbiW4iyEz0fEnZLOAEYjYgXwzxRNyjUULYEj+lfjyUtc1/OAbYGvlsfD74+IQ/pW6SlIXN/KSFzfa4G3Sfop8CzwoYj4Vf9qPXmJ63sScImkD1J0kxwzpDtxSPoKRZfeDuUxjw8DfwAQERdRHAM5GFgD/BZ4d1dff0jfNzMz65Kqdw2ZmVkbDgIzs5pzEJiZ1ZyDwMys5hwEZmY15yCwJJKelXSbpDskfVXSizv8+990WP5SSYe1eHxE0ifL28dI+lR5+3hJf97w+Ks6eb0J6vHfypE8b5O0TdNzY+/JTyT9WNKbysfnjzeK5CTrcKKkn0laXb7W+ZL+oEvL7vRz+QdJ/7u8Pd5n9LFyhMwvNDz2Z5LeP/UaWw4OAkv1ZEQsjojdgWeA4xufLC90yf59iojRiHhfi8cvioixDc8xQFeCADgK+D/luj/Z9NzYe/JfgVOAs7v0ms+TdDzwNmCfiNgDeCOwEdhmwj/sE0kvAd4UEXsC0yTtUQboMcCn+1o5G5eDwCbjh8Cu5Z7vXZI+DfwYmCfpyHLP9Q5JH2v8I0n/WO45f1fS7PKxYyXdXO7p/mtTS+MAST+U9HNJf1KW31/SFoPIje2plnuoI8Dl5d76Mkn/1lDuQElfb/H3b1Uxrv1qFWPDbyXpvcA7gNMlXd7mPdkO+HWL5T7fainvXyNp//L22yTdWL4nX5W0bYvlnkox1PIjABHxTEScExGPlcv4TcOyD5N0aXn7UkmfUTFe/1pJbynX666xMg1/1+nnMpHngBmSRBFWvwM+BHwyIn6XuAzrMQeBdUTFeEwHAavLh15LMTzuEoof/ceAPwYWA2+UtLwsNxP4cUS8AfgBxZWTAF+PiDeWe9V3UYwfM2Y+xcB4y4CLJG3drn4R8TVgFDgqIhZTXJH5urENHMUVmf/StE5bA5cCh5d73dMpNr6fo7i0/0MRcVSLl9umDJufAZ+jGPI6iYohIE4DDijfk1HgxKYys4Bty3GDJuNlFJ/FB4FvUgzC93pgD0mLyzKT+VzGFRGPA/9KMS/CfRSj+b4xIr4xyXWwHnAQWKptJN1GscG6n2JoDoBflOOjQ9FtcX1EbCqH9L6cYsINKPYUryxvfwn4o/L27uVe/2qKbpjXN7zmVRHxXETcQzFmzm6dVroccuCLFMMVvxTYly2H334tcF9E/Ly8f1lDvScy1jW0G7AU+EK5J5xiH4rJVG4o39d3Ac0zxokXjqj6P8rgWTd2PKKNb5brvxp4KCJWlyNz3kkRsjC5z2VCEXFu+b6cRBGOp0t6r6SrJJ2WuhzrnUqPNWRd9WS5h/28cpv3RONDHSxvbAN3KbA8In4i6RiK8Vaay4x3P9W/UOwRPwV8tQypRlOeiCgibiz38mc3PbWZF+5wjbVqBFwXEUdOsMzHJD0haUFE3BcR1wLXll1jM8aKtVj2mLHRZZ9ruD12f7zffsrnkkTFIIcAPwc+ERFvlnSFpIVluNuAcIvAumkV8BZJO0iaRjFe/A/K515EMborwDuBH5W3ZwEPlmfBNHe/vF3SiyTtQmrGBrIAAAFtSURBVDFl4d2J9Xi8XC4AEfEAxZC9p1Fs4Jr9DJgvadfy/p811DuJpN0oBkdrHuRtHbC4XI95FDNvQTEa6n5jrynpxZJe02LRZwOfKVszlC2Oxg3+Q5JeVx6o/9NO6lyazOeS6qPA6RSDp00rH3sO6OiMM8vPLQLrmoh4UNIpwPcp9nhXNvQNPwG8XtItFP3Gh5eP/z1FgPyCogtjVsMi76bYIO8IHB8RTyX2vFxKcUzhSWDf8myfy4HZEfHTFvV+StK7KUZlnU4xBPJFCa8z1l1Gub7viohnm+p4A0Vf+WrgDoqD6kTEpnJP+yuStirLnkax99zoMxQbzlWSngZ+Uy7z1vL5k4FrKGavuoNidNlOTOZzaas8NnRzGcKUB8VXA7dHxE86rKNl5tFHrRbKM3dujYh/blvYrGYcBFZ55d7uE8CBQz4rm1kWDgIzs5rzwWIzs5pzEJiZ1ZyDwMys5hwEZmY15yAwM6u5/w9cKcjI0TFJVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(blue_prob_list,gini_list)\n",
    "plt.xlabel('Probability of Blue Gumball %')\n",
    "plt.ylabel('Gini')\n",
    "plt.title('Gini Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Entropy Curve')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(blue_prob_list,ent_list)\n",
    "plt.xlabel('Probability of Blue Gumball %')\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('Entropy Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification\n",
    "\n",
    "Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction\n",
    "\n",
    "<img src='img/rf-01.jpeg' />\n",
    "\n",
    "Random forest builds multiple decision trees, \n",
    "\n",
    "- each tree uses repeated data from the training set\n",
    "- each tree also will have random set of features\n",
    "\n",
    "Consider the below dataset,\n",
    "<img src='img/dt-3.png' />\n",
    "\n",
    "We divide the input dataset into multiple samples. In the above dataset, you can see the row r6 is part of all 3 samples. Samples can be taken row level or column level(take x1,x2,x3 and y)\n",
    "\n",
    "We run decision tree on each of the samples and see how the y variable is classified.\n",
    "\n",
    "We do majority voting and choose the final prediction as the one said by the majority (for this reason we usually go for odd number of occurrences so that a clear majority comes out)\n",
    "\n",
    "This technique is called Random Forest. Random Forest is a <b> Bagging </b> technique\n",
    "\n",
    "Step 1 of the Random Forest model building is to create the bootstrap data set\n",
    "\n",
    "While picking the data for the models, we choose to avoid picking a few rows , these rows are called out of the bag samples. This OOB samples will be used for testing the models and accuracy will be checked for.\n",
    "\n",
    "Step 2 Create a decision tree using the bootstrapped dataset, but only use a random subset of variables(columns) at each step\n",
    "\n",
    "Step 3 We repeat Steps 1 & 2 and build quite a lot of decision trees\n",
    "\n",
    "Step 4 Use Out of bag data samples to test the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "To understand bootstrap, suppose it were possible to draw repeated samples (of the same size) from the population of interest, a large number of times. Then, one would get a fairly good idea about the sampling distribution of a particular statistic from the collection of its values arising from these repeated samples. The idea behind bootstrap is to use the data of a sample study at hand as a “surrogate population”, for the purpose of approximating the sampling distribution of a statistic; i.e. to resample (with replacement) from the sample data at hand and create a large number of “phantom samples” known as bootstrap samples.\n",
    "In other words, We randomly sample with replacement from the n known observations. We then call this a bootstrap sample. Since we allow for replacement, this bootstrap sample most likely not identical to our initial sample. Some data points may be duplicated, and others data points from the initial may be omitted in a bootstrap sample.\n",
    "An Example:\n",
    "The following numerical example will help to demonstrate how the process works. If we begin with the sample 2, 4, 5, 6, 6, then all of the following are possible bootstrap samples:\n",
    "\n",
    "    2 ,5, 5, 6, 6\n",
    "    4, 5, 6, 6, 6\n",
    "    2, 2, 4, 5, 5\n",
    "    2, 2, 2, 4, 6\n",
    "    2, 2, 2, 2, 2\n",
    "    4, 6, 6, 6, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classification\n",
    "\n",
    "<img src='img/bag-boost.png' /> \n",
    "\n",
    "In Bagging we run multiple algorithms parallely and then make a decision from the outputs.\n",
    "\n",
    "In Boosting, we run multiple algorithms sequentially. Output of first algorithm will be fed as input to the next and so on.\n",
    "\n",
    "Bagging is an approach to ensemble learning that is based on bootstrapping. In short, given a training set, we produce multiple different training sets (called bootstrap samples), by sampling with replacement from the original dataset. Then, for each bootstrap sample, we build a model. The results in an ensemble of models, where each model votes with the equal weight. Typically, the goal of this procedure is to reduce the variance of the model of interest (e.g. decision trees).\n",
    "\n",
    "<b> Advantages of Bagging techniques includes </b>\n",
    "\n",
    "Improving the stability and accuracy of machine learning algorithms used in statistical classification and regression. \n",
    "\n",
    "It also reduces variance and helps to avoid overfitting. \n",
    "\n",
    "Although it is usually applied to decision tree methods, it can be used with any type of method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording\n",
    "\n",
    "https://drive.google.com/drive/folders/14wmJ6qDqLQb63PMVdaGf1QqppjWeZcD_\n",
    "\n",
    "## References\n",
    "\n",
    "https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\n",
    "\n",
    "https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb\n",
    "\n",
    "https://medium.com/datadriveninvestor/tree-algorithms-id3-c4-5-c5-0-and-cart-413387342164"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "193.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
