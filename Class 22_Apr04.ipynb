{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Day 22 - Class </h1>\n",
    "\n",
    "## Linear Regression Contd..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias v/s Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised machine learning an algorithm learns a model from training data.\n",
    "\n",
    "The goal of any supervised machine learning algorithm is to best estimate the mapping function (f) for the output variable (Y) given the input data (X). The mapping function is often called the target function because it is the function that a given supervised machine learning algorithm aims to approximate.\n",
    "\n",
    "There is a tradeoff between a model’s ability to minimize bias and variance. Gaining a proper understanding of these errors would help us not only to build accurate models but also to avoid the mistake of <b>overfitting</b> and <b>underfitting</b>.\n",
    "\n",
    "<b>Bias</b> are the simplifying assumptions made by a model to make the target function easier to learn.\n",
    "\n",
    "<b>Variance</b> is the amount that the estimate of the target function will change if different training data was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias are the simplifying assumptions made by a model to make the target function easier to learn.\n",
    "\n",
    "Generally, linear algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.\n",
    "\n",
    "<b>Low Bias</b>: Suggests less assumptions about the form of the target function.\n",
    "          e.g. - Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
    "<b>High-Bias</b>: Suggests more assumptions about the form of the target function.\n",
    "          e.g. - Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "          \n",
    "Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance is the amount that the estimate of the target function will change if different training data was used.\n",
    "\n",
    "The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to have some variance. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.\n",
    "\n",
    "Machine learning algorithms that have a high variance are strongly influenced by the specifics of the training data. This means that the specifics of the training have influences the number and types of parameters used to characterize the mapping function.\n",
    "\n",
    "<b>Low Variance</b>: Suggests small changes to the estimate of the target function with changes to the training dataset.\n",
    "                     e.g. - Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "                     \n",
    "<b>High Variance</b>: Suggests large changes to the estimate of the target function with changes to the training dataset.\n",
    "                     e.g. - Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
    "                     \n",
    "Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over fitting / Under fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, \n",
    "- Overfitting will work well in training data, but in test/new data it will fail to predict well.\n",
    "- Underfitting will not work well in training data as well, as it's trying to make simplified assumptions.\n",
    "\n",
    "<img src='img/over-under-fitting.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the complexity of the model increases (e.g. increase in the number of x variables), we can achieve low bias but variance will go high and vice-versa for variance. We need to keep a balance between the two for the model to perform the best and to reduce the error\n",
    "\n",
    "\n",
    "\n",
    "<img src='img/over-under-fitting-balance.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance.\n",
    "\n",
    "Linear machine learning algorithms often have a high bias but a low variance.\n",
    "Nonlinear machine learning algorithms often have a low bias but a high variance.\n",
    "\n",
    "The parameterization of machine learning algorithms is often a battle to balance out bias and variance.\n",
    "\n",
    "Below are two examples of configuring the bias-variance trade-off for specific algorithms:\n",
    "\n",
    "The k-nearest neighbors algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbors that contribute t the prediction and in turn increases the bias of the model. \n",
    "\n",
    "Each algorithem will have such parameters (number of neighbours in this case) which the data scientist is expected to tune to achieve best performance. This process is called <b>hyperparameter tuning</b>.\n",
    "    \n",
    "There is no escaping the relationship between bias and variance in machine learning.\n",
    "\n",
    "Increasing the bias will decrease the variance.\n",
    "Increasing the variance will decrease the bias.\n",
    "\n",
    "There is a trade-off at play between these two concerns and the algorithms you choose and the way you choose to configure them are finding different balances in this trade-off for your problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a supervised technique for classification (as opposed to K-Means which is an unsupervised technique for clustering).\n",
    "\n",
    "When we say increase in the number of neighbours, this is what it means,\n",
    "\n",
    "- We pick a row and see which neighbours is closest to the row. This is done by comparing distance between this row and every other row in the dataset.\n",
    "- Per KNN, the row with the least distance is the closest neighbour. \n",
    "- When we increase the number of neighbours, what it does is , instead of picking 1 nearest neighbour, it picks the 'n' nearest neighbours and find the mean of those neighbours to do the prediction. \n",
    "- This works for continous variables and class variables. For class variables, instead of taking the mean , we can find the mode and use the mode for doing the prediction. This is one reason why we should go for odd number of neighbours when we change the hyperparameter. Because if we choose even number then it causes confusion to the model to find the majority\n",
    "\n",
    "\n",
    "With this understanding, we can clearly see that KNN can also be used for null value treatment\n",
    "<pre>\n",
    "<b>x1  x2  x3  x4  y</b>\n",
    "1   5   4   3   2\n",
    "8   4   1   15  12\n",
    "7   3   2   1   14\n",
    "4   12      9   2\n",
    "2   1   15  10  1\n",
    "</pre>\n",
    "\n",
    "Notice in row value for x3 in row 4 is missing. Pick x1,x2,x4 and then find the nearest neighbour of row 4 , let's imagine row 5 is the nearest neighbour then we fill the null vaulue with 15.\n",
    "\n",
    "Sir explained we can also use KNN for outlier treatment, its not clear to me how. \n",
    "\n",
    "<img src='img/knn.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of changing the beta coefficients in such a way as to find an optimum acceptable bias and variance is regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge and Lasso Regression are types of Regularization techniques\n",
    "- Regularization techniques are used to deal with overfitting and when the dataset is large\n",
    "- Ridge and Lasso Regression involve adding penalties to the regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. If you’ve heard of them before, you must know that they work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques. The key difference is in how they assign penalty to the coefficients:\n",
    "\n",
    "- Ridge Regression:\n",
    "    <br>Performs L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\n",
    "    <br>Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
    "- Lasso Regression:\n",
    "    <br>Performs L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients\n",
    "    <br>Minimization objective = LS Obj + α * (sum of absolute value of coefficients)\n",
    "\n",
    "Note that here ‘LS Obj’ refers to ‘least squares objective’, i.e. the linear regression objective without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is used when the number of predictor variables in a set exceeds the number of observations, or when a data set has multicollinearity (correlations between predictor variables).\n",
    "\n",
    "Least squares regression isn’t defined at all when the number of predictors exceeds the number of observations; It doesn’t differentiate “important” from “less-important” predictors in a model, so it includes all of them. This leads to overfitting a model and failure to find unique solutions. Least squares also has issues dealing with multicollinearity in data. Ridge regression avoids all of these problems. It works in part because it doesn’t require unbiased estimators; While least squares produces unbiased estimates, variances can be so large that they may be wholly inaccurate. Ridge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values. \n",
    "\n",
    "Ridge regression belongs a class of regression tools that use L2 regularization. The other type of regularization, L1 regularization, limits the size of the coefficients by adding an L1 penalty equal to the absolute value of the magnitude of coefficients. This sometimes results in the elimination of some coefficients altogether, which can yield sparse models. L2 regularization adds an L2 penalty, which equals the square of the magnitude of coefficients. All coefficients are shrunk by the same factor (so none are eliminated). Unlike L1 regularization, L2 will not result in sparse models.\n",
    "\n",
    "A tuning parameter (λ) controls the strength of the penalty term. When λ = 0, ridge regression equals least squares regression. If λ = ∞, all coefficients are shrunk to zero. The ideal penalty is therefore somewhere in between 0 and ∞.\n",
    "\n",
    "Note : Ridge penalizes using sum of squared coefficients \n",
    "\n",
    "<img src='img/ridge.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "The acronym “LASSO” stands for Least Absolute Shrinkage and Selection Operator.\n",
    "\n",
    "Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models. On the other hand, L2 regularization (e.g. Ridge regression) doesn’t result in elimination of coefficients or sparse models. This makes the Lasso far easier to interpret than the Ridge. \n",
    "\n",
    "A tuning parameter, λ controls the strength of the L1 penalty. λ is basically the amount of shrinkage:\n",
    "\n",
    "    When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
    "    As λ increases, more and more coefficients are set to zero and eliminated (theoretically, when λ = ∞, all coefficients are eliminated).\n",
    "    As λ increases, bias increases.\n",
    "    As λ decreases, variance increases.\n",
    "\n",
    "If an intercept is included in the model, it is usually left unchanged. \n",
    "\n",
    "Note : Lasso penalizes using the sum of their absolute values \n",
    "\n",
    "<img src='img/lasso.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net includes both L-1 and L-2 norm regularization terms. This gives us the benefits of both Lasso and Ridge regression. It has been found to have predictive power better than Lasso, while still performing feature selection. We therefore get the best of both worlds, performing feature selection of Lasso with the feature-group selection of Ridge.\n",
    "\n",
    "Elastic Net comes with the additional overhead of determining the two lambda values for optimal solutions.\n",
    "\n",
    "<img src='img/elasticnet.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Fold validation\n",
    "- Leave One Out Cross Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definition of Linear Regression\t\n",
    "    - Simple Linear Regression\n",
    "    - Multiple Linear Regression\n",
    "- Linear Regression – Intuition ( Explain to a common Man)\n",
    "- Algorithm Assumptions\n",
    "    - Linearity \n",
    "    - Heteroskedacity\n",
    "    - Normality \n",
    "    - Multicollinearity \n",
    "- Coding the Assumptions\n",
    "- Matrix Way of Representing Data\n",
    "- Preprocessing \n",
    "    - Null values\n",
    "    - Outliers\n",
    "    - Standard Scalar\n",
    "    - Label Encoders\n",
    "- Solving beta coefficients\n",
    "    - Matrix Method \n",
    "    - Hypothesis Testing\n",
    "- OLS ( Ordinary least squares method)\n",
    "- Applying Coefficients on Test Data\n",
    "- Model Validation\n",
    "    - Cost function\n",
    "    - Loss function \n",
    "    - MSE\n",
    "    - RMSE\n",
    "    - MAE\n",
    "    - R2 \n",
    "- Model Validation\n",
    "    - K Fold\n",
    "    - LOOCV\n",
    "- Transformations\n",
    "    - Log Transformation for Skewed Data\n",
    "    - Linear Transformation\n",
    "    - Y=mx\n",
    "    - Non Linear Transformation\n",
    "        - Y=log(x),sin(x),cos(x)\n",
    "- Model Regularization\n",
    "    - Ridge\n",
    "    - Lasso\n",
    "    - Elastic Net\n",
    "- Coding Linear Regression\n",
    "    - SKLEAR\n",
    "    - STATS Models\n",
    "- Interpreting the outputs\n",
    "    - STATS Models\n",
    "- Interpretation of Linear Regression\n",
    "    - Top 5 Coeff\n",
    "    - Bottom 5 Coeff\n",
    "- Model Optimization \n",
    "    - Gradient Descent\n",
    "- ANN ( Artificial Neural Network Representation of Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Video Recordings @ https://drive.google.com/open?id=19ouVNpaVrl8dnAeZ1L2YFUg9I9DePxwI\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
