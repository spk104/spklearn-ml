{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Day 20 - Class </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sin, Cos, Tan\n",
    "sin (theta) = opposite side / hypotonuse\n",
    "\n",
    "cos (theta) = adjacent side / hypotonuse\n",
    "\n",
    "tan (theta) = opposite side / adjacent side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.981223826364186"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.sin(40) * 55\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-36.681593390874404"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.cos(40) * 55\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant of a matrix\n",
    "\n",
    "Determinant is a number that represents the how much the area is squished or scaled after a transformation. The scaling factor , the factor by which any linear transformation scales/squishes the area is called the determinant\n",
    "\n",
    "The determinant will be 3 if the area of the area is scaled out by 3 times.\n",
    "\n",
    "<img src='img/determinant-01.png' />\n",
    "\n",
    "<img src='img/determinant-02.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra\n",
    "\n",
    "<img src='img/linear-alg-01.png' />\n",
    "\n",
    "Rank of a matrix -> Number of dimensions in the output after the transformation\n",
    "\n",
    "Column space ->  the column space (also called the range or image) of a matrix A is the span (set of all possible linear combinations) of its column vectors. The column space of a matrix is the image or range of the corresponding matrix transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen values and Eigen vectors\n",
    "\n",
    "An Eigen vector is such a vector that stays in the line span after a transformation is applied, just that it will be stretched of squished by a scalar. The scalar here is called Eigen value\n",
    "\n",
    "<img src='img/eigen-01.png'/>\n",
    "\n",
    "<img src='img/eigen-02.png'/>\n",
    "\n",
    "<img src='img/eigen-03.png'/>\n",
    "\n",
    "<img src='img/eigen-04.png'/>\n",
    "\n",
    "This will always be true if vector v is zero, but that's not what we are looking for. The only way it's possible that the product of a matrix with a non-zero vector is zero , is when the transformation associated with that matrix squishes the space into a lower dimension i.e. and that squishification corresponds to a zero determinant.\n",
    "\n",
    "<img src='img/eigen-05.png'/>\n",
    "\n",
    "Once you figure out Lambda (Eigen value) you could solve the equation and figure out the Eigen vector\n",
    "\n",
    "Its possible that a transformation might not have an eigen vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/eigen-values.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigen values are used in PCA kind of ML concepts\n",
    "\n",
    "In the below image x is the vector in hand .. it's multipled with matrix A (also known as transformation) .. the result nothing but the same vector which got scaled by 5\n",
    "\n",
    "- 5 is the eigen value here\n",
    "- x is the eigen vector\n",
    "\n",
    "dimensionality redcution is the concept here.. basically a 2*2 matrix (A) is reduced to a 1*1 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/eigen-value-concept.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of eigenvalues and eigenvectors\n",
    "\n",
    "- A square matrix A and its transpose have the same eigenvalues.\n",
    "    EigenValue(Matrix A) = EigenValue(Transpose of A)\n",
    "\n",
    "    We have that\n",
    "\n",
    "    det(AT – λI)\n",
    "\n",
    "\n",
    "    = det(AT – λIT)\n",
    "    = det(A –λI)T\n",
    "    = det(A –λI)\n",
    "\n",
    "    so any solution of det(A –λI) = 0 is a solution of det(A –λI)T = 0 and vice versa. Thus A and AT have the same eigenvalues.\n",
    "\n",
    "    The matrices A and AT will usually have different eigenvectors.\n",
    "\n",
    "- The eigenvalues of a diagonal or triangular matrix are its diagonal elements.\n",
    "\n",
    "    Suppose the matrix A is diagonal or triangular. If you subtract λ's from its diagonal elements, the result A – λI is still diagonal or triangular. Its determinant is the product of its diagonal elements, so it is just the product of factors of the form (diagonal element – λ). The roots of the characteristic equation must then be the diagonal elements.\n",
    "\n",
    "- An n x n matrix is invertible if and only if it doesn't have 0 as an eigenvalue.\n",
    "\n",
    "    An n x n matrix A has an eigenvalue 0 if and only if det(A – 0I) = 0, i.e. if and only if det(A) = 0. Since A is invertible if and only if detA ≠ 0, A is invertible if and only if 0 is not an eigenvalue of A.\n",
    "    \n",
    "- If a matrix A has eigenvalue λ with corresponding eigenvector x, then for any k = 1, 2, ... , Ak has eigenvalue λk corresponding to the same eigenvector x.\n",
    "\n",
    "    Suppose the matrix A has eigenvalue λ with eigenvector x, i.e. suppose that Ax = λx. Then A2x = A(Ax) = A(λx) = λ(Ax) = λ(λx) = λ2x. Multiply by more A's to get A3x = λ3x, A4x = λ4x and so on.\n",
    "\n",
    "- If A is an invertible matrix with eigenvalue λ corresponding to eigenvector x, then A–1 has eigenvalue λ–1 corresponding to the same eigenvector x.\n",
    "\n",
    "     Multiply the equation Ax = λx by λ–1A–1:\n",
    "          λ–1A–1(Ax) = λ–1A–1λx,\n",
    "    i.e.\n",
    "         λ–1x = A–1x.\n",
    "    Thus A–1 has eigenvalue λ–1 corresponding to the same eigenvector x.\n",
    "\n",
    "\n",
    "- Eigenvectors of a matrix A with distinct eigenvalues are linearly independent.\n",
    "\n",
    "    Suppose the statement is not true, i.e. suppose that A has a linearly dependent set of eigenvectors each with a different eigenvalue. \"Thin out\" this set of vectors to get a linearly independent subset v1, v2, ..., vk, with distinct eigenvalues λ1, λ2, ..., λk.\n",
    "\n",
    "    Suppose u is one of the eigenvectors you thinned out because it was linearly dependent on the others:\n",
    "                 u = c1v1 + c2v2 + ... + ckvk       *\n",
    "    for some scalars c1, c2, ..., ck.\n",
    "\n",
    "    First multiply * by A:\n",
    "\n",
    "    Au\n",
    "\n",
    "\n",
    "    = A(c1v1 + c2v2 + ... + ckvk)\n",
    "    = c1Av1 + c2Av2 + ... + ckAvk\n",
    "    = c1λ1v1 + c2λ2v2 + ... + ckλkvk.\n",
    "\n",
    "    Since u is also an eigenvector, Au = λu for some eigenvalue λ, so this equation gives\n",
    "             λu = c1λ1v1 + c2λ2v2 + ... + ckλkvk.           **\n",
    "\n",
    "    Now multiply * by λ:\n",
    "             λu = λc1v1 + λc2v2 + ... + λckvk .          ***\n",
    "\n",
    "    Subtract *** from ** to get\n",
    "\n",
    "    0\n",
    "\n",
    "\n",
    "    = (c1λ1 – λc1)v1 + c2λ2 – λc2)v2 + ... + (ckλk – λck)vk\n",
    "    = c1(λ1 – λ)v1 + c2(λ2 – λ)v2 + ... + ck(λk – λ)vk.\n",
    "\n",
    "    Since the vi's are linearly independent, ci(λi – λ) = 0 for all i = 1, 2, ..., k. Since the eigenvalues (including λ) are all different, ci = 0 for all i. But this implies (from equation *) that u = 0, which is impossible since u is an eigenvector.\n",
    "\n",
    "    The original assumption must be false, i.e. it is not possible to have a linearly dependent set of eigenvectors with distinct eigenvalues; any eigenvectors with distinct eigenvalues must be linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/prop-3-eigen.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonal matrix is a square matrix such that\n",
    "inv(A) = A.T\n",
    "\n",
    "Mode of eigen values generated from an orthogonal matrix is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace and Norm of a matrix\n",
    "<img src='img/trace-norm.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.mathsisfun.com\n",
    "\n",
    "https://www.mathsisfun.com/algebra/eigenvalue.html\n",
    "\n",
    "3blue1brown\n",
    "\n",
    "http://thejuniverse.org/PUBLIC/LinearAlgebra/MATH-232/Unit.12/Presentation.2/Section12A/properties.html\n",
    "\n",
    "## TODO\n",
    "\n",
    "Read\n",
    "- Gaussian elimination\n",
    "- Row echelon form\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
